<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Szymon Antoniak *">
<meta name="author" content="Michał Krutul">
<meta name="author" content="Maciej Pióro">
<meta name="author" content="Jakub Krajewski">
<meta name="author" content="Jan Ludziejewski">
<meta name="author" content="Tomasz Odrzygóźdź">
<meta name="author" content="Marek Cygan ‡">
<meta name="author" content="Sebastian Jaszczur †">
<meta name="dcterms.date" content="2023-09-29">

<title>llm-random - Mixture of Tokens</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QHXJCKXK7M"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QHXJCKXK7M', { 'anonymize_ip': true});
</script>
<script charset="utf-8" src="https://cdn.plot.ly/plotly-2.20.0.min.js"> </script>
<style>

  .MK {
  color: brown;
  }
  .MK::before {
  content: "[MK] ";
  }

  .SA {
  color: goldenrod;
  }
  .SA::before {
  content: "[SA] ";
  }

  .MP {
  color: teal;
  }
  .MP::before {
  content: "[MP] ";
  }

  .JK {
  color: coral;
  }
  .JK::before {
  content: "[JK] ";
  }

  .JL {
  color: green;
  }
  .JL::before {
  content: "[JL] ";
  }

  .TO {
  color: purple;
  }
  .TO::before {
  content: "[TO] ";
  }

  .MC {
  color: red;
  }
  .MC::before {
  content: "[MC] ";
  }

  .SJ {
  color: blue;
  }
  .SJ::before {
  content: "[SJ] ";
  }
</style>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">llm-random</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Mixture of Tokens</h1>
                      </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-heading">Affiliations</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author">Szymon Antoniak * </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              IDEAS NCBR
            </p>
          <p class="affiliation">
              University of Warsaw
            </p>
        </div>
        <div class="quarto-title-meta-contents">
      <p class="author">Michał Krutul </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              IDEAS NCBR
            </p>
          <p class="affiliation">
              University of Warsaw
            </p>
        </div>
        <div class="quarto-title-meta-contents">
      <p class="author">Maciej Pióro </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              IDEAS NCBR
            </p>
          <p class="affiliation">
              University of Warsaw
            </p>
        </div>
        <div class="quarto-title-meta-contents">
      <p class="author">Jakub Krajewski </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              IDEAS NCBR
            </p>
          <p class="affiliation">
              University of Warsaw
            </p>
        </div>
        <div class="quarto-title-meta-contents">
      <p class="author">Jan Ludziejewski </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              IDEAS NCBR
            </p>
          <p class="affiliation">
              University of Warsaw
            </p>
        </div>
        <div class="quarto-title-meta-contents">
      <p class="author">Tomasz Odrzygóźdź </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              IDEAS NCBR
            </p>
          <p class="affiliation">
              University of Warsaw
            </p>
        </div>
        <div class="quarto-title-meta-contents">
      <p class="author">Marek Cygan ‡ </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University of Warsaw
            </p>
          <p class="affiliation">
              Nomagic
            </p>
        </div>
        <div class="quarto-title-meta-contents">
      <p class="author">Sebastian Jaszczur † </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              IDEAS NCBR
            </p>
          <p class="affiliation">
              University of Warsaw
            </p>
        </div>
      </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 29, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="test" class="level1">
<h1>TEST</h1>
<p><span class="SA">new text</span><del>old text</del></p>
<p><span class="MK">new text</span><del>old text</del></p>
<p><span class="MP">new text</span><del>old text</del></p>
<p><span class="JK">new text</span><del>old text</del></p>
<p><span class="JL">new text</span><del>old text</del></p>
<p><span class="TO">new text</span><del>old text</del></p>
<p><span class="SJ">new text</span><del>old text</del></p>
<p><span class="MC">new text</span><del>old text</del></p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Mixture of Experts (MoE) architectures have recently garnerned considerable attention for their ability to increase the size of Transformer architectures while keeping the computational cost of training and inference constant. The most successful MoE approaches achieve this by activating only subsets of a very large feed forward layer for each processed token (the alternative subsets of parameters are often called <em>experts</em>).</p>
<p>This technique comes at a cost, though: the operation of choosing the most suitable experts for a given token is discrete, and as such learning how to choose is difficult; the models are known to suffer from issues such as training instability, expert under- and overload and more. While some of those problems can be alleviated with e.g.&nbsp;the use of various auxiliary losses or reduced initialisation scale, it is certain that existing MoE techniques are more difficult and less intuitive to train.</p>
<p>With this motivation, we propose Mixture of Tokens: a new, fully-differentiable type of architecture that retains all efficiency benefits of MoE and alleviates the aformentioned problems by <em>mixing tokens</em> before feeding them into the FeedForward layer instead of routing tokens to experts, in effect allowing all experts to learn from all tokens <span class="SJ">(I don’t agree)</span>. <span class="SA">allowing the model to simultaneously learn from all token-expert combinations ||| what u think??</span> Crucially, this technique is fully compatible with both masked and causual LLM training.</p>
</section>
<section id="motivation" class="level1">
<h1>Motivation</h1>
<section id="scaling-language-models" class="level2">
<h2 class="anchored" data-anchor-id="scaling-language-models">Scaling Language Models</h2>
<p>Numerous works <span class="citation" data-cites="kaplan2020scaling hoffmann2022training">(<a href="#ref-kaplan2020scaling" role="doc-biblioref">Kaplan et al. 2020</a>; <a href="#ref-hoffmann2022training" role="doc-biblioref">Hoffmann et al. 2022</a>)</span> have studied benefits of increasing the size of language models for their performance. However, this scaling comes at a very high computational cost <span class="citation" data-cites="strubell2019energy">(<a href="#ref-strubell2019energy" role="doc-biblioref">Strubell, Ganesh, and McCallum 2019</a>)</span>. Therefore, efficient use of computational resources and network parameters is often recognized as one of the most important challenges in the area <span class="citation" data-cites="rae2022scaling jaszczur2021sparse nawrot2022hierarchical">(<a href="#ref-rae2022scaling" role="doc-biblioref">Rae et al. 2022</a>; <a href="#ref-jaszczur2021sparse" role="doc-biblioref">Jaszczur et al. 2021</a>; <a href="#ref-nawrot2022hierarchical" role="doc-biblioref">Nawrot et al. 2022</a>)</span>. One of the most promising research directions in this area is conditional computation, meaning methods that involve activating only a subset of parameters for a given input. In the context of language models, this idea has been most successfully implemented in Mixture of Experts (MoE), leading to huge increases in the quality of model performance.</p>
</section>
<section id="mixture-of-experts" class="level2">
<h2 class="anchored" data-anchor-id="mixture-of-experts">Mixture of Experts</h2>
<p>The core idea in MoE is to increase model size without incurring additional computational costs. Roughly speaking, is is done by replacing the FeedForward layer standard for Transformer architectures with a (potentially very large) set of experts. In this new setup, a given token is processed only by a small subset of experts, and that choice is decided by a small network called a <em>controller</em> or <em>router</em>. The change in model quality due to this increase in model size has been shown to behave predictably with respect to parameter count and inference compute <span class="citation" data-cites="clark2022unified">(<a href="#ref-clark2022unified" role="doc-biblioref">Clark et al. 2022</a>)</span>. More detailed backgorund and explanation of variants of the MoE architecture is presented in the <a href="#sec-back">next section</a>.</p>
</section>
<section id="limitations-of-current-approaches" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-current-approaches">Limitations of current approaches</h2>
<p>Despite many promising results, wider adoption of Mixture has been limited by a number of restrictions of the architecture. and difficulties in the training. Among these, the most important are:</p>
<ul>
<li><strong>Training insability.</strong> Numerous works <span class="citation" data-cites="fedus2022switch du2022glam">(<a href="#ref-fedus2022switch" role="doc-biblioref">Fedus, Zoph, and Shazeer 2022</a>; <a href="#ref-du2022glam" role="doc-biblioref">Du et al. 2022</a>)</span> have reported difficulties in training MoE due to frequent instabilities. This is most likely connected with the discrete nature of the technique: the operation of choosing top-k most relevant tokens/experts in non-differentiable. <del>To train the network using gradient descent, there is a need for <em>tricks</em> to imitate the gradient, which come at the cost of the worse quality of training</del><span class="SA">We hypothesise that various techniques used for training the cotnroller with gradient descent result in lower training stability</span>.</li>
<li><strong>Load imbalance.</strong> Typically, in MoE we set the maximum <em>capacity</em> of each expert. However, we are not able to efficiently restrict the choice of the routing network to assign tokens in a perfectly balanced way. This leads to <em>token droping</em> (when some tokens are not processed at all) and <em>expert under-specialization</em> (when some experts receive less data and are therefore not trained enough). <span class="SA">there is work by Basil that does some balancing for that. Will liink later</span></li>
<li><strong>Information leak.</strong> Some of the most successfull methods in the area process tokens from different positions in a sequence together (i. e. by comparing scores of all tokens in a batch). This imposes information leak, and makes it impossible to use such techniques in autoregressive decoding, covering a large part of use cases in modern language models. <!--[SIMON ANOTHER LINK!!!!!!]--></li>
</ul>
<p>We want to address these problems using our techique. <!-- JK - tu można dorzucić nierówne traktowanie przykładów (niektóre przykłądy dostają więcej compute). Można też dać, ze niektóre techniki nie proponują rozwiązania, które byłoby użyteczne dla generative decoding, ale trzeba by dać related work powyżej --></p>
<!-- JK - byłoby bardziej logicze dać tę sekcję pod related work -->
<!--Without mentioning papers 
- obecne podejścia są niestabilne
- mamy idealny balancing token -> expert

with training stability, management of expert _load_ (how many tokens are routed to each expert) that lead to time overheads, mode collapse (all tokens routed to a small portion of the expert set), leaving some tokens unprocessed, problems with expert exploration and specialization/redundancy, low performance of discrete operations on accelerators and finally communication costs in parallel training.-->
</section>
</section>
<section id="sec-back" class="level1">
<h1>Background</h1>
<p>In the context of language models, Mixture of Experts has been originally proposed in <span class="citation" data-cites="shazeer2017outrageously">(<a href="#ref-shazeer2017outrageously" role="doc-biblioref">Shazeer et al. 2017</a>)</span>. Basic overview of the technique is presented on the diagram below: instead of processing each token with the standard Feed-Forward layer, we choose a</p>
<iframe frameborder="0" style="width:100%;height:400px;" src="https://viewer.diagrams.net/?tags=%7B%7D&amp;target=blank&amp;highlight=0000ff&amp;edit=_blank&amp;layers=1&amp;nav=1&amp;title=generic_moe.drawio#R7Zpdc6IwFIZ%2FjZd2CAGEy11duze705l29uMyhaiZjcQN8au%2FfoMkAoGtxYKlY%2FVCcxJCOO%2BTnENgAMfL3S1Hq8U3FmE6sK1oN4CTgW0D6DnyJ7XslSWAbmaZcxIpW264J09YGS1lXZMIJ6WGgjEqyKpsDFkc41CUbIhzti03mzFaPusKzXHFcB8iWrX%2BJJFYZFbfHuX2r5jMF%2FrMwAuymiXSjdWVJAsUsW3BBL8M4JgzJrJ%2Fy90Y09R72i%2FZcdP%2F1B4HxnEsXnLA9GE8fVjez%2FmPp6fh5MftN7reDNVgN4iu1QUPbI%2FK%2Fj7PmOw29Stl%2FFDj%2FV2nQ%2F08sCH00m%2FR5M3T3%2Bn0uz5cjiPrIatRLhB77VfO1nGE06EBWb1dEIHvVyhMa7cSJWlbiCVV1TNC6fg4EBgh7M9CaU8EZ39wocaSH89To5%2BiJaEpd18x3WBBQnQcSNFrypEbzAXeFUzKi7eYLbHge9lE1eojFNJDYEEl8TYnZKQ8uyjA4ahmSDE5P3adyyb%2FKOUaqGg7NTIaDsdx9CmdD7IUUpQkJCz7OBfEkiXpHb7%2FlRZuXF38rVoeCpNdseVkr0qJQFyoWQwPtVGhVNHLmaTf52SR%2Fc2xOE0wjkrzuCpeQRvXqmqjbRxTJMimPPvrBFNnuGPkME8UG55VZiNwDc0TtuYhVkcVZ%2Bupjnyjo8wxlY4O%2FBwv%2BxVIuW0j1ToaPZU8Xw5eqzmwRhcW3WspHBQo0ca0g2FyEPyTbADAapdFDiOGyEAuUaAU8%2BahxGocSmYz7IW1oSQaBY%2BWpQauQAWgnQgCbXN229UI4tUga3cWQUZ9iSDdRwxNeU%2FWD%2BgaMPjemevH0KksIBeOGhA2wuiRsvDPw4LEUs8IJYtjPligKrXfISEwjw8W28q50Bl5DX07IgrwydLvQk2OXlrYt0ask5WmJPWaCnqN%2Bc3Ufkf8mmHLD87k1%2FGDG9dYGM2%2Buga4WSb9AfAHwMWegrcHuFne3iHAHcB4zqS4BoDNDCKwzgXYenuA%2FRqAX7UlBa5gTwr4vduUgq3vLdpXIKQ96p2QuuP2hIRXICT0%2BickaFtI5wqEdNz%2BCdksyYtZjDvI73SeZj2TpzmuV8jUhtaNJR317D2MLNxhTqSjMDdTyWPu2PRRRFeJoJ5RJ%2FecnH49pnBAhWlza%2FHFN%2B3A3HQC4LIZo1OXMX7c8nRzy3OSdJ2%2F94T0Fm%2FuQfXe6MLbq05dSm2STilZJfh0lEXJKnthYkZ2KcLnYNMgHXKhuUxYoBJFYZUD2FEMdeuy2v%2BvGm%2FxgPN0%2BOlXVIFucBMUPr6p%2BLkPRqHrml2ZuVXHM8%2BtS517mXG9bZjQsry7hCgYVRIiY%2BV5Max%2BhXsNy6VgtT9gfQmseofq3cHaXvYOR44JKzC47xrWZo%2BMrxdW%2F33CCquvS54Nq1NZWUFbK6ss5q%2FiZs3zN5rhl38%3D"></iframe>
<p>The technique was further simplified by <span class="citation" data-cites="fedus2022switch">(<a href="#ref-fedus2022switch" role="doc-biblioref">Fedus, Zoph, and Shazeer 2022</a>)</span>, proposing to send each token to only <em>one</em> expert with the highest score produced by the routing network. In both cases there was a need to use additional auxiliary loss components to encourage exploration and mitigate load imbalance across experts. More recently, <span class="citation" data-cites="zhou2022mixtureofexperts">(<a href="#ref-zhou2022mixtureofexperts" role="doc-biblioref">Zhou et al. 2022</a>)</span> proposed to compose the routing network from another perspective and choose <em>Top-k tokens for each expert</em> instead. This introduces potentially higher disparity in the computation among various parts of the batch, however eliminates the problem of load imbalance between experts. The difference between both approaches, <em>token choice</em> and <em>expert choice</em>, is illustrated on the diagram below.</p>
<iframe frameborder="0" style="width:100%;height:512px;" src="https://viewer.diagrams.net/?tags=%7B%7D&amp;highlight=0000ff&amp;edit=_blank&amp;layers=1&amp;nav=1&amp;page-id=77xPu9R-wu4rnPo6aUtL&amp;title=Untitled%20Diagram.drawio#Uhttps%3A%2F%2Fdrive.google.com%2Fuc%3Fid%3D1jvew0ft43zpaG-iCk7Ob2ilJw7VjsyeZ%26export%3Ddownload"></iframe>
<p>Concurrently to our work, <span class="citation" data-cites="puigcerver2023sparse">(<a href="#ref-puigcerver2023sparse" role="doc-biblioref">Puigcerver et al. 2023</a>)</span> proposed a continuous variant of Mixture of Experts for the Vision Transformer, as such limited to encoder-only models.</p>
<!-- JK- czy były inne soft wersje moe? -->
<!-- [This needs to be quite comprehensive, what we see below unnecessarily focuses on Expert Choice and Softmoe]{.SA}
In recent years, there has been a growing interest in methods of increasing the number of parameters in neural networks while maintaining their computational demands. A major advance in eliminating some problems with traditional MoE approaches is Expert Choice (here biblio). Our work has been inspired by their approach - MoT can be seen as an Expert Choice (as opposed to *Token Choice*) kind of method.

There is also concurrent work (Softmoe) nvestigating ideas similar to ours in the vision domain. It is, however, limited to encoder-only models. It is worth noting that earlier, other fully-differentiable MoE approaches had been proposed (biblio copied from Softmoe). /* jakoś inaczej sformułowac: However, expert-mixing strategies presented in those works incur significant computational overhead. Also, they are token choice instead of expert choice, so different than us. */ -->
</section>
<section id="method-sj" class="level1">
<h1>Method [SJ]</h1>
<p>In this section we provide an overview of our proposed method. More detailed derivation of all operations can be found in the next section. [T U T A J L I N K]</p>
<p><span class="SJ">Link to diagrams: https://drive.google.com/file/d/1jvew0ft43zpaG-iCk7Ob2ilJw7VjsyeZ/view?usp=sharing</span> <!-- 
[Move this paragraph to background]{.SJ}
The general idea of MoT is to run multiple experts which independently aggregate and process tokens. The most common (and original) form of MoE, token-choice, operates from the perspective of a single token. [This paragraph will be more detailed in background section.]{.SJ} However, MoT is more similar to expert-choice, where it is an expert who decides which tokens to process.

[comparison of both token-choice and expert-choice]{.SJ}

{{< include diagrams/_vanillamoes.qmd >}}  --></p>
<p>In MoT, each expert works independently from one another. <span class="SJ">The only dependence is during experts’ outputs aggregation, but it’s a matter of rescaling each output before adding it to the residual, so we will skip it/explain it in later section.</span> Therefore, to explain the inner workings of MoT we will focus on a single expert. See the diagram below.</p>
<iframe width="780" height="500" src="https://viewer.diagrams.net/index.html?tags=%7B%7D&amp;highlight=0000ff&amp;edit=_blank&amp;layers=1&amp;nav=1&amp;page-id=abJAIyk8VqZmFR_1QRqh&amp;title=Untitled%20Diagram.drawio#Uhttps%3A%2F%2Fdrive.google.com%2Fuc%3Fid%3D1jvew0ft43zpaG-iCk7Ob2ilJw7VjsyeZ%26export%3Ddownload#%7B%22pageId%22%3A%22abJAIyk8VqZmFR_1QRqh%22%7D
" title="Figure"></iframe>
<p>This single expert works as a normal feedforward layer<span class="SJ">(maybe a diagram of lin1-&gt;relu-&gt;lin2?)</span>, except for a merger and an emitter placed to process the input and the output, respectively. Each merger takes a group of tokens and computes a weighted average of them <del>(according to weights described in later section)</del>.</p>
<p><del>Now we get to the part of computing weights.</del> The weights are the same <span class="SJ">(are they really?)</span><span class="MK">It depends on a version</span> for both merger and emitter.</p>
<p>How can we apply this technique in decoder? We need a kind of grouping that will not leak information into future tokens. We can achieve that by grouping tokens by position.</p>
<iframe frameborder="0" style="width:100%;height:400px;" src="https://viewer.diagrams.net/?tags=%7B%7D&amp;highlight=0000ff&amp;edit=_blank&amp;layers=1&amp;nav=1&amp;page-id=fbuRVmtAR91k7QgiWp5u&amp;title=MoT_diagrams_sj.drawio#Uhttps%3A%2F%2Fdrive.google.com%2Fuc%3Fid%3D1jvew0ft43zpaG-iCk7Ob2ilJw7VjsyeZ%26export%3Ddownload"></iframe>


<!-- ## Implementation
In the first phase of our MoT Layer, we partition batch sequences into token gropus of equal size. 

Then for each group, we calculate a two sets of weights, named _merge_ and _emit_. Each set of weights sum to 1.0, signifying the percentage of each token to utilize.

For every group, using its _merge_ weights, we merge belonging tokens into a single token. Following this, we process all merged tokens with linear experts' weights and a relu activation function. [Komentarz: To zdecydowanie trzeba lepiej napisać]{.MK} 
Employing _emit_ weights, we redistribute resultant token after the experts' layer back to a group of tokens. Lastly, we join the groups to align with the Feed Forward layer input dimensions.

## Encoder vs Decoder
In encoder-only models,   ach token may attend to the whole input sequence, whereas decoder-only models only allow attending to the tokens that came before a particular token to simulate the autoregressive decoding used during model inference. This is crucial for the training of decoder-only models, as allowing a token to attend to future tokens would constitute a data leak, preventing successful training.
A major problem potentially facing any token-mixing strategy is the possibility of a data leak, making the strategy inappropriate for use with decoder-only models. In MoT, the tokens are mixed across batch, i.e. with other tokens at the same position in other samples in the same training batch. We don't consider other strategies which would be more suitable in the context of encoders. -->
</section>
<section id="experiments" class="level1">
<h1>Experiments</h1>
<section id="experimental-setup" class="level2">
<h2 class="anchored" data-anchor-id="experimental-setup">Experimental setup</h2>
<p>We train a standard GPT-like model on the language modeling task using cross entropy loss on the C4 dataset <span class="citation" data-cites="2019t5">(<a href="#ref-2019t5" role="doc-biblioref">Raffel et al. 2019</a>)</span>. The size of the dataset is big enough so that we never sample the same example twice.</p>
<p>For POC experiments, we chose model size identical to <strong>BERT-mini</strong>:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Model
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>4 layers</li>
<li>256 embedding dimension</li>
<li>1024 hidden dimension</li>
<li>4 attention heads</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Training:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>250K training steps</li>
<li>256 context length</li>
<li>256 batch size</li>
<li>lr warmup for the first 1% of training</li>
<li>cosine scheduling to 0.1 of peak lr at the end of training</li>
</ul>
</div>
</div>
<p>The learning rate was tuned separately for both our model and the baseline:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Rate:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Dense baseline: 4e-3</li>
<li>Mixture of Tokens: 2e-3</li>
</ul>
</div>
</div>
<p>Our model trains on group size of 32, with 32x more parameters in the FeedForward layer compared to the baseline, distributed among 512 experts.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>


<meta charset="utf-8">

    <div>                            <div id="d65831fa-2e08-438f-bdff-39a5dc6e2b94" class="plotly-graph-div" style="height:100%; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("d65831fa-2e08-438f-bdff-39a5dc6e2b94")) {                    Plotly.newPlot(                        "d65831fa-2e08-438f-bdff-39a5dc6e2b94",                        [{"name":"Baseline","x":[1000.0,2000.0,3000.0,4000.0,5000.0,6000.0,7000.0,8000.0,9000.0,10000.0,11000.0,12000.0,13000.0,14000.0,15000.0,16000.0,17000.0,18000.0,19000.0,20000.0,21000.0,22000.0,23000.0,24000.0,25000.0,26000.0,27000.0,28000.0,29000.0,30000.0,31000.0,32000.0,33000.0,34000.0,35000.0,36000.0,37000.0,38000.0,39000.0,40000.0,41000.0,42000.0,43000.0,44000.0,45000.0,46000.0,47000.0,48000.0,49000.0,50000.0,51000.0,52000.0,53000.0,54000.0,55000.0,56000.0,57000.0,58000.0,59000.0,60000.0,61000.0,62000.0,63000.0,64000.0,65000.0,66000.0,67000.0,68000.0,69000.0,70000.0,71000.0,72000.0,73000.0,74000.0,75000.0,76000.0,77000.0,78000.0,79000.0,80000.0,81000.0,82000.0,83000.0,84000.0,85000.0,86000.0,87000.0,88000.0,89000.0,90000.0,91000.0,92000.0,93000.0,94000.0,95000.0,96000.0,97000.0,98000.0,99000.0,100000.0,101000.0,102000.0,103000.0,104000.0,105000.0,106000.0,107000.0,108000.0,109000.0,110000.0,111000.0,112000.0,113000.0,114000.0,115000.0,116000.0,117000.0,118000.0,119000.0,120000.0,121000.0,122000.0,123000.0,124000.0,125000.0,126000.0,127000.0,128000.0,129000.0,130000.0,131000.0,132000.0,133000.0,134000.0,135000.0,136000.0,137000.0,138000.0,139000.0,140000.0,141000.0,142000.0,143000.0,144000.0,145000.0,146000.0,147000.0,148000.0,149000.0,150000.0,151000.0,152000.0,153000.0,154000.0,155000.0,156000.0,157000.0,158000.0,159000.0,160000.0,161000.0,162000.0,163000.0,164000.0,165000.0,166000.0,167000.0,168000.0,169000.0,170000.0,171000.0,172000.0,173000.0,174000.0,175000.0,176000.0,177000.0,178000.0,179000.0,180000.0,181000.0,182000.0,183000.0,184000.0,185000.0,186000.0,187000.0,188000.0,189000.0,190000.0,191000.0,192000.0,193000.0,194000.0,195000.0,196000.0,197000.0,198000.0,199000.0,200000.0,201000.0,202000.0,203000.0,204000.0,205000.0,206000.0,207000.0,208000.0,209000.0,210000.0,211000.0,212000.0,213000.0,214000.0,215000.0,216000.0,217000.0,218000.0,219000.0,220000.0,221000.0,222000.0,223000.0,224000.0,225000.0,226000.0,227000.0,228000.0,229000.0,230000.0,231000.0,232000.0,233000.0,234000.0,235000.0,236000.0,237000.0,238000.0,239000.0,240000.0,241000.0,242000.0,243000.0,244000.0,245000.0,246000.0,247000.0,248000.0,249000.0,250000.0],"y":[6.74839453125,5.4187265625,4.9736796875,4.735296875,4.630875,4.56698828125,4.519,4.4851484375,4.45952734375,4.43978515625,4.42198828125,4.4113125,4.39663671875,4.38164453125,4.37398046875,4.363734375,4.3577890625,4.34557421875,4.3384140625,4.32876171875,4.3243203125,4.319109375,4.3143203125,4.30580078125,4.2994453125,4.2956015625,4.292625,4.28841796875,4.28327734375,4.2778046875,4.277015625,4.272265625,4.2683046875,4.2671953125,4.262171875,4.26112109375,4.258078125,4.25455859375,4.25040625,4.24560546875,4.2448046875,4.24388671875,4.243,4.237890625,4.235296875,4.23173046875,4.2287109375,4.22712109375,4.22437109375,4.22453125,4.2206796875,4.21865625,4.21700390625,4.2149140625,4.21329296875,4.21173828125,4.20691015625,4.20617578125,4.20566015625,4.2051171875,4.199109375,4.20054296875,4.19573046875,4.19511328125,4.19466015625,4.1915859375,4.190609375,4.18851171875,4.18603125,4.1866171875,4.1823828125,4.1841171875,4.17944140625,4.18012890625,4.17614453125,4.17441015625,4.1732734375,4.171921875,4.1710234375,4.16890625,4.16660546875,4.1675703125,4.16252734375,4.161875,4.16170703125,4.159328125,4.15687890625,4.15765625,4.1536953125,4.15349609375,4.151734375,4.15009765625,4.15083203125,4.1477578125,4.14693359375,4.14521484375,4.14371875,4.140640625,4.1391875,4.1400859375,4.13821875,4.136171875,4.1346953125,4.13290234375,4.13382421875,4.1321171875,4.127845703125,4.12960546875,4.1271328125,4.124853515625,4.126546875,4.12116015625,4.118875,4.1194375,4.11716796875,4.117326171875,4.11452734375,4.11478515625,4.11265625,4.1118515625,4.110716796875,4.10695703125,4.105974609375,4.1069453125,4.1039609375,4.104115234375,4.10241796875,4.10298046875,4.100599609375,4.09512890625,4.09860546875,4.094466796875,4.09503515625,4.093646484375,4.08973828125,4.09240234375,4.09222265625,4.088080078125,4.08678515625,4.085791015625,4.082439453125,4.082064453125,4.08147265625,4.080451171875,4.08063671875,4.07994921875,4.07630078125,4.07576953125,4.074875,4.07455078125,4.073796875,4.071392578125,4.072060546875,4.06762890625,4.066365234375,4.0654765625,4.064796875,4.06455078125,4.0633359375,4.061970703125,4.060384765625,4.0594140625,4.059236328125,4.057814453125,4.056111328125,4.05688671875,4.055232421875,4.05441015625,4.052625,4.05313671875,4.05051171875,4.049529296875,4.0474765625,4.04734765625,4.046658203125,4.04436328125,4.0429375,4.044142578125,4.04340234375,4.039486328125,4.0435625,4.04140234375,4.0408046875,4.0386015625,4.0380625,4.037591796875,4.032896484375,4.0354765625,4.035083984375,4.03198828125,4.032712890625,4.030740234375,4.02956640625,4.029828125,4.0271875,4.025908203125,4.0264765625,4.024916015625,4.02585546875,4.024517578125,4.024865234375,4.02329296875,4.022279296875,4.021298828125,4.021123046875,4.021474609375,4.02086328125,4.01965234375,4.01898046875,4.01805859375,4.017619140625,4.01712109375,4.015287109375,4.01531640625,4.016392578125,4.014390625,4.013958984375,4.0118671875,4.0142265625,4.01219140625,4.011998046875,4.011248046875,4.010705078125,4.01103125,4.01233984375,4.008513671875,4.01001953125,4.00755078125,4.008427734375,4.00748046875,4.006416015625,4.007978515625,4.005435546875,4.00546875,4.00516015625,4.00378515625,4.00555859375,4.0049765625,4.005669921875,4.005052734375,4.002826171875,4.003580078125,4.002212890625,4.00363671875,4.001841796875,4.00212890625,4.003541015625,4.002578125,4.0037109375,4.00257421875],"type":"scatter"},{"name":"Mixture of Tokens","x":[1000.0,2000.0,3000.0,4000.0,5000.0,6000.0,7000.0,8000.0,9000.0,10000.0,11000.0,12000.0,13000.0,14000.0,15000.0,16000.0,17000.0,18000.0,19000.0,20000.0,21000.0,22000.0,23000.0,24000.0,25000.0,26000.0,27000.0,28000.0,29000.0,30000.0,31000.0,32000.0,33000.0,34000.0,35000.0,36000.0,37000.0,38000.0,39000.0,40000.0,41000.0,42000.0,43000.0,44000.0,45000.0,46000.0,47000.0,48000.0,49000.0,50000.0,51000.0,52000.0,53000.0,54000.0,55000.0,56000.0,57000.0,58000.0,59000.0,60000.0,61000.0,62000.0,63000.0,64000.0,65000.0,66000.0,67000.0,68000.0,69000.0,70000.0,71000.0,72000.0,73000.0,74000.0,75000.0,76000.0,77000.0,78000.0,79000.0,80000.0,81000.0,82000.0,83000.0,84000.0,85000.0,86000.0,87000.0,88000.0,89000.0,90000.0,91000.0,92000.0,93000.0,94000.0,95000.0,96000.0,97000.0,98000.0,99000.0,100000.0,101000.0,102000.0,103000.0,104000.0,105000.0,106000.0,107000.0,108000.0,109000.0,110000.0,111000.0,112000.0,113000.0,114000.0,115000.0,116000.0,117000.0,118000.0,119000.0,120000.0,121000.0,122000.0],"y":[6.865790036678314,5.500211125850678,5.061204619407654,4.781460484981537,4.641498310565948,4.555344714164734,4.490929737091064,4.443757008075714,4.405196396350861,4.37313479423523,4.344426648139954,4.323108457088471,4.299607085704803,4.279431181430817,4.263306029796601,4.247776464939117,4.234482085704803,4.221364744663239,4.2098522415161135,4.196074651241302,4.187593363761902,4.177408302307129,4.170638454437256,4.159920306205749,4.15081564617157,4.145412252902984,4.139124257087707,4.131852756023407,4.123324332237243,4.11731966304779,4.113346085071564,4.107695070266724,4.101226757526398,4.098156608104706,4.091862853527069,4.088939291715622,4.08453405046463,4.080003803968429,4.07474255156517,4.069562127351761,4.0672049102783205,4.064178363561631,4.0609314668178556,4.056524780988693,4.053007598161697,4.047103298664093,4.043819102048874,4.0414996073246,4.038083200454712,4.03592233991623,4.032123554468155,4.029180178403855,4.027229268789291,4.024126539945603,4.02128267121315,4.018775345087051,4.0136179099082945,4.012527460336686,4.011893786191941,4.0094586644172665,4.003874507665635,4.00408109164238,3.9995777270793913,3.997126739740372,3.9969446184635165,3.9934199278354643,3.9913556628227234,3.9893966519832613,3.9867340905666353,3.986563229560852,3.9814727687835694,3.9834299783706664,3.978228116750717,3.9784797337055204,3.974903697013855,3.972870152950287,3.9708729276657104,3.9691721091270447,3.9676777884960175,3.9655341460704805,3.962736085653305,3.9628406882286074,3.958150596380234,3.9572064304351806,3.9572911744117736,3.9544747989177704,3.9525790514945984,3.9521092398166657,3.9483649508953094,3.948203722476959,3.945921738624573,3.9438791739940644,3.9440752167701723,3.942056930065155,3.9397456793785097,3.938746748685837,3.937531670331955,3.9344002854824067,3.931611962556839,3.932666870355606,3.9304284567832948,3.9282506439685823,3.9271608049869537,3.9252567477226257,3.9263192043304445,3.9234907367229463,3.9210682899951936,3.921439744234085,3.919512340307236,3.916478786468506,3.918222924232483,3.912615067720413,3.909725915670395,3.91176522898674,3.9081051626205445,3.9083814005851747,3.9069166042804717,3.9061199796199797,3.9035818481445315,3.903709605693817,3.9023211553096773,3.8991231470108034],"type":"scatter"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Training Loss vs Step","x":0.5},"shapes":[{"line":{"color":"red","dash":"dash","width":0.5},"type":"line","x0":61000.0,"x1":250000.0,"y0":4.00257421875,"y1":4.00257421875}],"annotations":[{"arrowcolor":"red","arrowhead":3,"arrowwidth":0.5,"ax":0.0,"ay":50,"font":{"color":"red","size":11},"showarrow":true,"text":"24% of training steps","x":117700.0,"y":4.00257421875}],"xaxis":{"range":[0,250000.0],"title":{"text":"training step"}},"yaxis":{"range":[3.5,5],"title":{"text":"loss"}}},                        {"responsive": true}                    )                };                            </script>        </div>


<p>Our Mixture of Tokens model attains the dense models’ final training loss in just 24% of steps.</p>


<meta charset="utf-8">

    <div>                            <div id="5ea53e61-ff4b-4404-9b32-9d7bc2d2d73b" class="plotly-graph-div" style="height:100%; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("5ea53e61-ff4b-4404-9b32-9d7bc2d2d73b")) {                    Plotly.newPlot(                        "5ea53e61-ff4b-4404-9b32-9d7bc2d2d73b",                        [{"name":"Baseline","x":[0.0,0.08762166666666667,0.17644361111111112,0.26527666666666666,0.3539075,0.4435047222222222,0.5321094444444444,0.6218075000000001,0.7105758333333333,0.7999666666666667,0.8883986111111112,0.9774852777777778,1.0673408333333334,1.1558666666666666,1.2442119444444444,1.3331069444444446,1.419881388888889,1.5081022222222222,1.5953297222222222,1.6828311111111112,1.7700372222222223,1.8575183333333334,1.9448580555555555,2.0327033333333335,2.121116944444444,2.2089166666666666,2.29659,2.385211388888889,2.473233333333333,2.559935,2.6478683333333333,2.7348616666666667,2.8230225,2.9098430555555552,2.998060277777778,3.0854044444444444,3.1721883333333336,3.2592933333333334,3.3472327777777777,3.433851388888889,3.521051388888889,3.609291666666667,3.6965080555555554,3.7832800000000004,3.8702430555555556,3.9576874999999996,4.044698333333334,4.1319525,4.218946944444444,4.306704444444445,4.393870277777778,4.481081388888889,4.568294444444445,4.655444444444444,4.742686388888888,4.8297897222222215,4.916908888888889,5.004116111111111,5.0910649999999995,5.178214166666667,5.265334999999999,5.3524344444444445,5.439638333333334,5.526818333333333,5.613704722222222,5.700817499999999,5.787931944444445,5.875058888888889,5.962191666666667,6.049612222222222,6.136620833333333,6.224062222222222,6.311106666666666,6.398553611111112,6.4857233333333335,6.572978055555556,6.6600463888888894,6.7472336111111115,6.834268333333334,6.921397222222222,7.00843111111111,7.095643055555556,7.182788888888889,7.2697433333333334,7.356875,7.443795,7.530955277777777,7.6179775,7.704945833333333,7.792298611111112,7.879179722222222,7.966483888888889,8.053386944444444,8.140718055555554,8.227731944444445,8.315153333333333,8.401953055555555,8.489047222222222,8.57599888888889,8.6630075,8.750449444444444,8.83744138888889,8.92451,9.01161361111111,9.09861861111111,9.185570555555554,9.27280861111111,9.359810833333334,9.446755555555555,9.533864999999999,9.620834166666667,9.707910833333333,9.794974722222221,9.8820975,9.969790833333334,10.059529166666668,10.148275277777778,10.237225555555556,10.325973055555554,10.413803611111112,10.500648055555555,10.588213055555556,10.67548861111111,10.762519166666667,10.850113055555555,10.937474166666666,11.025301944444445,11.11185888888889,11.199786666666668,11.2870425,11.374980555555556,11.462117222222224,11.549749166666667,11.636829444444444,11.724797222222222,11.811960555555554,11.899705555555556,11.987722777777778,12.074925555555556,12.162574166666667,12.250062777777778,12.337125277777778,12.42514888888889,12.511965277777778,12.59950138888889,12.687078055555556,12.77459138888889,12.862399166666668,12.949969166666667,13.038156388888888,13.12617388888889,13.214363888888888,13.302555555555555,13.390345833333335,13.477509722222223,13.565365,13.65278638888889,13.741053888888889,13.828498055555556,13.917450277777778,14.0049925,14.092954166666667,14.180871388888889,14.269054166666667,14.35682,14.444571944444446,14.532745555555556,14.621474444444443,14.70994611111111,14.798998888888889,14.88682,14.975245555555555,15.063854444444443,15.151840277777778,15.239972222222223,15.328056388888887,15.416480555555555,15.505451111111112,15.593709166666667,15.682551388888887,15.770987222222221,15.859359166666666,15.947549166666668,16.035676111111112,16.124238333333334,16.212366666666664,16.3009725,16.389469444444444,16.477648333333335,16.56602611111111,16.654208055555554,16.7425775,16.830786111111113,16.918509444444442,17.00650472222222,17.094911111111113,17.183713333333333,17.272284444444445,17.36049527777778,17.449826388888887,17.538273611111112,17.62639638888889,17.715115555555556,17.802473333333335,17.891330555555555,17.980198333333334,18.069332777777777,18.158273333333334,18.24664333333333,18.335828611111108,18.424611666666667,18.51265777777778,18.601486666666666,18.690482499999998,18.779454722222223,18.868869166666666,18.95748888888889,19.0462675,19.135793333333332,19.22511777777778,19.313645833333332,19.402723055555555,19.492199444444445,19.581441666666667,19.670004166666665,19.759194444444447,19.847888333333334,19.93624,20.025037777777776,20.11338388888889,20.202275,20.29097833333333,20.37863888888889,20.467515833333334,20.556151944444444,20.644791944444446,20.733088611111114,20.821204722222223,20.909725277777778,20.998593055555556,21.086310555555553,21.174205555555556,21.26254472222222,21.351176666666667,21.440162222222224,21.528179166666668,21.61698777777778,21.705759444444443,21.794127777777778,21.883115000000004],"y":[6.74839453125,5.4187265625,4.9736796875,4.735296875,4.630875,4.56698828125,4.519,4.4851484375,4.45952734375,4.43978515625,4.42198828125,4.4113125,4.39663671875,4.38164453125,4.37398046875,4.363734375,4.3577890625,4.34557421875,4.3384140625,4.32876171875,4.3243203125,4.319109375,4.3143203125,4.30580078125,4.2994453125,4.2956015625,4.292625,4.28841796875,4.28327734375,4.2778046875,4.277015625,4.272265625,4.2683046875,4.2671953125,4.262171875,4.26112109375,4.258078125,4.25455859375,4.25040625,4.24560546875,4.2448046875,4.24388671875,4.243,4.237890625,4.235296875,4.23173046875,4.2287109375,4.22712109375,4.22437109375,4.22453125,4.2206796875,4.21865625,4.21700390625,4.2149140625,4.21329296875,4.21173828125,4.20691015625,4.20617578125,4.20566015625,4.2051171875,4.199109375,4.20054296875,4.19573046875,4.19511328125,4.19466015625,4.1915859375,4.190609375,4.18851171875,4.18603125,4.1866171875,4.1823828125,4.1841171875,4.17944140625,4.18012890625,4.17614453125,4.17441015625,4.1732734375,4.171921875,4.1710234375,4.16890625,4.16660546875,4.1675703125,4.16252734375,4.161875,4.16170703125,4.159328125,4.15687890625,4.15765625,4.1536953125,4.15349609375,4.151734375,4.15009765625,4.15083203125,4.1477578125,4.14693359375,4.14521484375,4.14371875,4.140640625,4.1391875,4.1400859375,4.13821875,4.136171875,4.1346953125,4.13290234375,4.13382421875,4.1321171875,4.127845703125,4.12960546875,4.1271328125,4.124853515625,4.126546875,4.12116015625,4.118875,4.1194375,4.11716796875,4.117326171875,4.11452734375,4.11478515625,4.11265625,4.1118515625,4.110716796875,4.10695703125,4.105974609375,4.1069453125,4.1039609375,4.104115234375,4.10241796875,4.10298046875,4.100599609375,4.09512890625,4.09860546875,4.094466796875,4.09503515625,4.093646484375,4.08973828125,4.09240234375,4.09222265625,4.088080078125,4.08678515625,4.085791015625,4.082439453125,4.082064453125,4.08147265625,4.080451171875,4.08063671875,4.07994921875,4.07630078125,4.07576953125,4.074875,4.07455078125,4.073796875,4.071392578125,4.072060546875,4.06762890625,4.066365234375,4.0654765625,4.064796875,4.06455078125,4.0633359375,4.061970703125,4.060384765625,4.0594140625,4.059236328125,4.057814453125,4.056111328125,4.05688671875,4.055232421875,4.05441015625,4.052625,4.05313671875,4.05051171875,4.049529296875,4.0474765625,4.04734765625,4.046658203125,4.04436328125,4.0429375,4.044142578125,4.04340234375,4.039486328125,4.0435625,4.04140234375,4.0408046875,4.0386015625,4.0380625,4.037591796875,4.032896484375,4.0354765625,4.035083984375,4.03198828125,4.032712890625,4.030740234375,4.02956640625,4.029828125,4.0271875,4.025908203125,4.0264765625,4.024916015625,4.02585546875,4.024517578125,4.024865234375,4.02329296875,4.022279296875,4.021298828125,4.021123046875,4.021474609375,4.02086328125,4.01965234375,4.01898046875,4.01805859375,4.017619140625,4.01712109375,4.015287109375,4.01531640625,4.016392578125,4.014390625,4.013958984375,4.0118671875,4.0142265625,4.01219140625,4.011998046875,4.011248046875,4.010705078125,4.01103125,4.01233984375,4.008513671875,4.01001953125,4.00755078125,4.008427734375,4.00748046875,4.006416015625,4.007978515625,4.005435546875,4.00546875,4.00516015625,4.00378515625,4.00555859375,4.0049765625,4.005669921875,4.005052734375,4.002826171875,4.003580078125,4.002212890625,4.00363671875,4.001841796875,4.00212890625,4.003541015625,4.002578125,4.0037109375,4.00257421875],"type":"scatter"},{"name":"Mixture of Tokens","x":[0.0,0.1102025,0.22032138888888889,0.33063944444444443,0.4408547222222222,0.55116,0.6615341666666668,0.7717666666666667,0.8822316666666667,0.9925736111111111,1.1029411111111111,1.2137411111111112,1.324238888888889,1.4347152777777779,1.5451597222222222,1.655526388888889,1.765925,1.8763033333333334,1.9867336111111111,2.0971166666666665,2.2076133333333336,2.318064166666667,2.4284366666666664,2.5388622222222224,2.6492127777777776,2.7596202777777776,2.870053333333333,2.9804244444444445,3.0908752777777777,3.201222222222222,3.311739166666667,3.4221605555555556,3.532686388888889,3.643100555555556,3.7536358333333335,3.8640230555555557,3.974463611111111,4.084901111111111,4.195413333333333,4.305719722222222,4.416119722222223,4.526548611111111,4.636904722222222,4.7472297222222215,4.8576258333333335,4.967887777777778,5.078225277777778,5.188641388888889,5.298884722222223,5.409263055555556,5.519609444444445,5.629938888888889,5.740241944444445,5.850555277777778,5.960919722222223,6.0711772222222224,6.181602222222223,6.291845555555556,6.402167222222222,6.512474444444444,6.622768333333333,6.732979444444444,6.843288055555555,6.843288055555555,6.953314722222222,7.063289722222223,7.173268055555557,7.283314166666667,7.3932725,7.503346666666667,7.613356111111111,7.723425000000001,7.833574722222222,7.9436550000000015,8.053704999999999,8.163783055555555,8.273855833333334,8.383854722222223,8.49399611111111,8.604089722222222,8.71408138888889,8.824217500000001,8.934220833333333,9.044411666666665,9.154599166666667,9.264683611111112,9.374765277777778,9.4850125,9.595071111111112,9.705115000000001,9.815252222222224,9.925120833333335,10.035108055555556,10.1452525,10.255217777777778,10.365317500000002,10.475235,10.585409166666667,10.695249722222224,10.80515611111111,10.9150025,11.024954722222223,11.135028333333333,11.245003333333335,11.354947222222224,11.465120555555556,11.575130833333333,11.685120555555557,11.79513388888889,11.905186944444445],"y":[6.865790036678314,5.500211125850678,5.061204619407654,4.781460484981537,4.641498310565948,4.555344714164734,4.490929737091064,4.443757008075714,4.405196396350861,4.37313479423523,4.344426648139954,4.323108457088471,4.299607085704803,4.279431181430817,4.263306029796601,4.247776464939117,4.234482085704803,4.221364744663239,4.2098522415161135,4.196074651241302,4.187593363761902,4.177408302307129,4.170638454437256,4.159920306205749,4.15081564617157,4.145412252902984,4.139124257087707,4.131852756023407,4.123324332237243,4.11731966304779,4.113346085071564,4.107695070266724,4.101226757526398,4.098156608104706,4.091862853527069,4.088939291715622,4.08453405046463,4.080003803968429,4.07474255156517,4.069562127351761,4.0672049102783205,4.064178363561631,4.0609314668178556,4.056524780988693,4.053007598161697,4.047103298664093,4.043819102048874,4.0414996073246,4.038083200454712,4.03592233991623,4.032123554468155,4.029180178403855,4.027229268789291,4.024126539945603,4.02128267121315,4.018775345087051,4.0136179099082945,4.012527460336686,4.011893786191941,4.0094586644172665,4.003874507665635,4.00408109164238,3.9995777270793913,3.997126739740372,3.9969446184635165,3.9934199278354643,3.9913556628227234,3.9893966519832613,3.9867340905666353,3.986563229560852,3.9814727687835694,3.9834299783706664,3.978228116750717,3.9784797337055204,3.974903697013855,3.972870152950287,3.9708729276657104,3.9691721091270447,3.9676777884960175,3.9655341460704805,3.962736085653305,3.9628406882286074,3.958150596380234,3.9572064304351806,3.9572911744117736,3.9544747989177704,3.9525790514945984,3.9521092398166657,3.9483649508953094,3.948203722476959,3.945921738624573,3.9438791739940644,3.9440752167701723,3.942056930065155,3.9397456793785097,3.938746748685837,3.937531670331955,3.9344002854824067,3.931611962556839,3.932666870355606,3.9304284567832948,3.9282506439685823,3.9271608049869537,3.9252567477226257,3.9263192043304445,3.9234907367229463,3.9210682899951936,3.921439744234085,3.919512340307236,3.916478786468506],"type":"scatter"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Training Loss vs Time","x":0.5},"shapes":[{"line":{"color":"red","dash":"dash","width":0.5},"type":"line","x0":61000.0,"x1":250000.0,"y0":4.00257421875,"y1":4.00257421875},{"line":{"color":"red","dash":"dash","width":0.5},"type":"line","x0":6.622768333333333,"x1":21.883115000000004,"y0":4.00257421875,"y1":4.00257421875}],"annotations":[{"arrowcolor":"red","arrowhead":3,"arrowwidth":0.5,"ax":0.0,"ay":50,"font":{"color":"red","size":11},"showarrow":true,"text":"24% of training steps","x":117700.0,"y":4.00257421875},{"arrowcolor":"red","arrowhead":3,"arrowwidth":0.5,"ax":0.0,"ay":50,"font":{"color":"red","size":11},"showarrow":true,"text":"33% of training time","x":11.200872333333333,"y":4.00257421875}],"xaxis":{"range":[0,21.883115000000004],"title":{"text":"time (hours)"}},"yaxis":{"range":[3.5,5],"title":{"text":"loss"}}},                        {"responsive": true}                    )                };                            </script>        </div>


<p>If we measure actual training time, the final dense model loss is achieved in only 33% of time.</p>
</section>
<section id="co-tam-dalej" class="level2">
<h2 class="anchored" data-anchor-id="co-tam-dalej">Co tam dalej</h2>
<p>ContMoE vs dense vs expert choice (GPT-BERT mini) Each run was optimized for that many batches of given size Contmoe is better than dense</p>
</section>
</section>
<section id="next-steps" class="level1">
<h1>Next steps</h1>
<p>Coming up in the following weeks, we’re ready to take our work up a notch with a few advancements that we’re covering in this segment. <span class="SA">Casual in a way that kinda irks me</span></p>
<section id="scaling-up" class="level2">
<h2 class="anchored" data-anchor-id="scaling-up">Scaling Up</h2>
<p>In a previous section <todo check=""> we showcased results for a BERT-mini FLOP matched MoT. Our preliminary experiments are also suggesting promising outcomes for larger models. In upcoming weeks we aim to prepare comprehensive comparison of larger models with FLOP matched MoT models.</todo></p>
</section>
<section id="discretization" class="level2">
<h2 class="anchored" data-anchor-id="discretization">Discretization</h2>
</section>
</section>
<section id="section" class="level1">
<h1></h1>
<p>This is similar in spirit to the controller in expert-choice MoE - controller took a group of tokens and chose and passed only one of them (or top-k of them in general, but for now let’s assume k=1). Similarly, the emitter which processes the output of the expert layer, just scales the output of the layer by a respective token’s weight, and passes the resulting vector as the update for this token. Note this is, again, similart to expert choice - you can just image the weight being a one-hot encoding. <span class="SJ">Maybe another diagram showing one-hot encoding vs MoT</span></p>
<iframe frameborder="0" style="width:100%;height:593px;" src="https://viewer.diagrams.net/?tags=%7B%7D&amp;highlight=0000ff&amp;edit=_blank&amp;layers=1&amp;nav=1&amp;page-id=C2AunCsj_dl_T9ErqCql&amp;title=Untitled%20Diagram.drawio#Uhttps%3A%2F%2Fdrive.google.com%2Fuc%3Fid%3D1jvew0ft43zpaG-iCk7Ob2ilJw7VjsyeZ%26export%3Ddownload"></iframe>
</section>
<section id="section-1" class="level1">
<h1></h1>
<p>In base MoT setup tokens from various samples in the batch are mixed. This theoretically opens doors for being able to tease out the output of one batch sample by another. However, this may pave the way for potential issues. [<strong>TODO Simon: Dodać wykresik, że temperatura się sama wygasza; Napisać że brak miksowania między sekwencjami jest dobry dla industry</strong>]</p>
<p>To tackle this problem, we plan to conduct an experiment where final phase will involve a strategic, gradual reduction of temperature, while simultaneously maintaining a low loss - a process referred to as <em>temerature annealing</em>. The anticipated outcome is a discrete-routing model which promotes inference parallelization - a move expected to significantly boost the overall efficiency of our processes.</p>
<!-- ## Do not forget about a BERT
As our results in this post covers only GPT model, in future work we plan to compare results of MoT on BERT model.  -->
</section>
<section id="conclusions-sj" class="level1">
<h1>Conclusions SJ</h1>
<p>We have shown the preliminary results showing the promise of MoT improving both the stability of training and final performance of the model. More thorough experiments are underway at the moment and we will release the paper with more results in the coming weeks. In the meantime, you can contact us with any feedback you have at llm.random.team@gmail.com .</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-clark2022unified" class="csl-entry" role="listitem">
Clark, Aidan, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, et al. 2022. <span>“Unified Scaling Laws for Routed Language Models.”</span> <a href="https://arxiv.org/abs/2202.01169">https://arxiv.org/abs/2202.01169</a>.
</div>
<div id="ref-du2022glam" class="csl-entry" role="listitem">
Du, Nan, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, et al. 2022. <span>“GLaM: Efficient Scaling of Language Models with Mixture-of-Experts.”</span> <a href="https://arxiv.org/abs/2112.06905">https://arxiv.org/abs/2112.06905</a>.
</div>
<div id="ref-fedus2022switch" class="csl-entry" role="listitem">
Fedus, William, Barret Zoph, and Noam Shazeer. 2022. <span>“Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.”</span> <a href="https://arxiv.org/abs/2101.03961">https://arxiv.org/abs/2101.03961</a>.
</div>
<div id="ref-hoffmann2022training" class="csl-entry" role="listitem">
Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. <span>“Training Compute-Optimal Large Language Models.”</span> <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a>.
</div>
<div id="ref-jaszczur2021sparse" class="csl-entry" role="listitem">
Jaszczur, Sebastian, Aakanksha Chowdhery, Afroz Mohiuddin, Łukasz Kaiser, Wojciech Gajewski, Henryk Michalewski, and Jonni Kanerva. 2021. <span>“Sparse Is Enough in Scaling Transformers.”</span> <a href="https://arxiv.org/abs/2111.12763">https://arxiv.org/abs/2111.12763</a>.
</div>
<div id="ref-kaplan2020scaling" class="csl-entry" role="listitem">
Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. <span>“Scaling Laws for Neural Language Models.”</span> <a href="https://arxiv.org/abs/2001.08361">https://arxiv.org/abs/2001.08361</a>.
</div>
<div id="ref-nawrot2022hierarchical" class="csl-entry" role="listitem">
Nawrot, Piotr, Szymon Tworkowski, Michał Tyrolski, Łukasz Kaiser, Yuhuai Wu, Christian Szegedy, and Henryk Michalewski. 2022. <span>“Hierarchical Transformers Are More Efficient Language Models.”</span> <a href="https://arxiv.org/abs/2110.13711">https://arxiv.org/abs/2110.13711</a>.
</div>
<div id="ref-puigcerver2023sparse" class="csl-entry" role="listitem">
Puigcerver, Joan, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. 2023. <span>“From Sparse to Soft Mixtures of Experts.”</span> <a href="https://arxiv.org/abs/2308.00951">https://arxiv.org/abs/2308.00951</a>.
</div>
<div id="ref-rae2022scaling" class="csl-entry" role="listitem">
Rae, Jack W., Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, et al. 2022. <span>“Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher.”</span> <a href="https://arxiv.org/abs/2112.11446">https://arxiv.org/abs/2112.11446</a>.
</div>
<div id="ref-2019t5" class="csl-entry" role="listitem">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. <span>“Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.”</span> <em>arXiv e-Prints</em>. <a href="https://arxiv.org/abs/1910.10683">https://arxiv.org/abs/1910.10683</a>.
</div>
<div id="ref-shazeer2017outrageously" class="csl-entry" role="listitem">
Shazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. <span>“Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.”</span> <a href="https://arxiv.org/abs/1701.06538">https://arxiv.org/abs/1701.06538</a>.
</div>
<div id="ref-strubell2019energy" class="csl-entry" role="listitem">
Strubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. <span>“Energy and Policy Considerations for Deep Learning in NLP.”</span> <a href="https://arxiv.org/abs/1906.02243">https://arxiv.org/abs/1906.02243</a>.
</div>
<div id="ref-zhou2022mixtureofexperts" class="csl-entry" role="listitem">
Zhou, Yanqi, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon. 2022. <span>“Mixture-of-Experts with Expert Choice Routing.”</span> <a href="https://arxiv.org/abs/2202.09368">https://arxiv.org/abs/2202.09368</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>