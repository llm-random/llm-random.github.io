[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/neuron_recycling/index.html",
    "href": "posts/neuron_recycling/index.html",
    "title": "Neuron Recycling",
    "section": "",
    "text": "Sparse neural networks have garnered attention due to their theoretical promise of lowered computational demands and memory savings. However, to this date, the theoretical gains have largely failed to materialize due to the lack of hardware support for this kind of models. In this work, we explore the idea of neuron recycling which is inspired by pruning - a method often employed to induce sparsity in neural networks. We also present lessons we have learned along the way.\n\nIntroduction\nPruning is a well-established technique used to sparsify neural networks. It relies on the fact that typically a large part of a trained neural network can be masked without impacting the accuracy of the network, albeit often requiring additional fine-tuning in order to regain some lost performance. Despite multiple works proposing various neuron-selection criteria for pruning, magnitude-based pruning remains a viable option. The Lottery Ticket Hypothesis is a major finding on the way to explain how the initialization impacts neural networks. The main point of the LTH is that through iterative pruning, performant subnetworks depending on the initialization can be found in neural networks. Those well-initialized network fragments are the namesake of LTH (the “lottery tickets”). Although some notions of the original LTH paper have been challenged, it has remained the subject of active research and a motivation for our work.  By combining the two ideas (pruning and LTH) we arrive at a new potential technique for raising neural network performance. If we are able to remove parts of the network without hurting the performance (pruning) and the fitness of a part of a network is determined at initialization, perhaps we could re-initialize the unnecessary network parts (i. e. draw more “lottery tickets”), leading to a better-performing network.\n\n\nPreliminaries\nBefore we move to the presentation of our experiments and findings, let’s first discuss the training setup, define key terminology, and go over the basics.\n\nModel and training setup\nIn our project, we are focusing on the Transformer, since it’s a major architecture across different domains. For the specific type of the model, we are working on encoder-only BERT . Taking into consideration available computational resources and expected iteration time (we wanted to try as many options as possible), we decided to opt for the BERT Medium configuration (with \\(d_\\text{model}=512\\) and \\(8\\) attention heads). We focus on the feed-forward layer, because it is the most computationally demanding part of commonly-used transformer models and, in large models, it contains the majority of the parameters. At the same time, the amount of research focusing on the attention mechanism is overwhelming, suggesting that the feed-forward layer is a relatively unexplored area.\nWe trained the model for \\(80{,}000\\) steps (around compute-optimal point), with Adam, using batch size of \\(256\\) and learning rate of \\(0.001\\).\n\nIn the following part of this post, we will often use the terms of neuron and magnitude. Below are the definitions we employ.\n\nNeuron  In the Transformer, feed-forward layer consists of two linear layers, with a nonlinearity in between. The first layer maps the input vector from \\(d_\\text{model}\\) to \\(d_\\text{ff}\\) dimension, and the second one from \\(d_\\text{ff}\\) back to \\(d_\\text{model}\\). Typically, \\(d_\\text{ff}\\) is four times greater than \\(d_\\text{model}\\). By neuron, we will understand all weights interacting with a particular coordinate in the hidden, \\(\\mathbb{R}^{d_\\text{ff}}\\) space. In the torch implementation, a neuron’s weights are the parameters in a given row of the first feed-forward matrix and in the corresponding column in the second one.\nMagnitude  To calculate magnitude of a weight, we will use its absolute value. As the magnitude of a neuron we will use value of the expression \\(M=\\sqrt{\\sum{w_{in}^2} \\sum{w_{out}^2}}\\). \n\n\n\nPruning\nPruning is a technique used to induce sparsity and decrease the parameter count in a neural network. In simple terms, it means deleting the least important neurons (structured pruning) or weights (unstructured pruning). A typical implementation realizes this by either multiplying the output of the deleted neurons by 0 or setting the weights of the neuron to 0. A widely-used proxy for the importance of a neuron or weight is its magnitude. You may find it counterintuitive, but we can even remove the whole FF Layer and the model will still work. This is because model can learn to represent the same transformation using attention.\n\nBelow we present a plot with loss curves of the model gradually pruned at the FF layer, starting in step \\(10{,}000\\), such that the layer is completely masked in the end of the training. As a comparison, we also add regular model and the one without feed-forward layer.\n\n\n\n\n                                                    \n\n\nInterestingly, the effect of pruning can’t be visible for a significant fraction of the training time. It’s also worth noting that in the end the model without FF Layer performs slightly better than the pruned one. This is because in the first case, Attention was trained to adjust  from the very beginning of the training.\n\n\nThe goal\n\n\nThe end-goal of the project was to create a method that would allow us to make better use of the parameters in the feed-forward layer. In this context, a natural question arises - against what baseline should our results be compared? To answer this question, we trained the model with differing dimensionalities of the feed-forward layer. The results are presented below. The true BERT Medium configuration has \\(d_\\text{ff}=2048\\), and, as expected, the model’s performance drops when the \\(d_\\text{ff}\\) is decreased and increases when the \\(d_\\text{ff}\\) is increased.\n\n\n\n\n                                                    \n\n\n\n\n\nUnderstanding neuron magnitudes\nOne of the key inspirations for our work was structured pruning, where neuron magnitude is often chosen as the measure of significance. We were interested in how this metric evolves during the training process. At first, we thought a histogram of neuron magnitudes would exhibit a normal distribution. However, our experiments showed something different. The following graph shows evolution of neuron magnitudes throughout the training process.\n\n\n\n\n    \n\n\n\n    \n        \n        \n    \n\n\n\n\nIn the early stages of training, it’s clear to see the neurons split into two unique groups, one featuring much lower magnitudes than the other. This finding opens up a multitude of discussion topics. It can be speculated that the neurons belonging to the group with smaller magnitudes potentially don’t hold much importance. There’s an option to consider eliminating them. However, it’s also possible that these minor neurons, though activated seldom, play a critical role in specific tasks.\nNotably, we noticed that this discrepancy does not happen in the last layers of the network. As an example, below you can examine magnitudes for neurons in the 8th FF layer of the network.\n\n\n\n\n    \n\n\n\n    \n        \n        \n    \n\n\n\n After examining these experiments, we were trying to understand why in the early layers we observed two distinct groups of neurons, categorized by their magnitudes. One possible explanation is that certain parts of the network receive a smaller signal and are slower to improve in training. We designed an experiment to check that. We periodically froze all parts of the network except for the feed-forward component and continued to train it for several batches of data. We hypothesized that in this scenario, weaker neurons might catch up, resulting in a more even distribution. We called this procedure overtraining  feed-forward layer. It’s important to note that this approach is impractical and computationally heavy, but we wanted to use it for the purpose of illustration. The results are depicted in the following plot.\n\n\n\n\n                                                    \n\n\n\n\nWe can see that the group of “weaker” neurons has moved to the right after performing additional training of the FF part. However, neurons still form two distinct groups: overtraining the whole layer is not enough for the weaker ones to catch up. In the next experiment, we have examined the scenario of retraining only small magnitude neurons, only large magnitude neurons and random subsets. How does it affect the performance? The results are depicted on the following plot.\n\n\n\n\n                                                    \n\n\n\nOvertraining only the smallest neurons yields the best results when compared to reinforcing high-magnitude ones. Notably, overtraining the small ones gives similar gains in performance to working on the entire layer! Contrarily, ampifying the highest ones gives gains similar to no overtraining at all. This provides a compelling argument in favor of our technique. \nSo far, we have performed a series of experiments on relatively small architectures. We were curious to see how our observations would translate to well-established, large-scale foundation models like BERT Large or T5.\n\n\n\n\n                                                    \n\n\n\n\n\n                                                    \n\n\nThere is a clear difference between the plots above. Magnitudes in T5 seem similar to those in our smaller models, while BERT presents a more balanced distribution. What could account for these variations? We discovered that the use of weight decay plays a significant role. This simple but widely used technique has an important impact on the distribution. \nThese findings support the idea of exploring neuron recycling and offer a good foundation for further experiments. In the next sections, we will present results on this topic and share our insights.\n\n\nRecycling\nThe central part of our work was a method we called neuron recycling. The whole process boils down to three phases, repeated periodically: training, selection and reinitialization.\n\n\n\n\n\n\nIn the training phase, the model is trained to predict masked tokens (masked language modelling).\nIn the selection phase, the least important neurons are determined, where the baseline criterion is neuron magnitude\nIn the reinitialization phase, new weights are assigned to neurons.\n\nAlthough this procedure is conceptually simple, it allows for many degrees of freedom. Here are some choices that can be made:\n\nThe number of training steps before consecutive selection / reinitialization phases\nThe percentage of recycled neurons\nSelection / reinitialization strategies\n\nAfter examining the pruning literature, we have found that the simple magnitude-based approach works best in most cases . It is also easy to implement and computationally efficient. This approach is also grounded on our experiments. Below we present the training curves for the model pruned gradually using different criterions: high/low magnitude and random neurons.\n\n\n\n\n                                                    \n\n\n\nAs you can see, removing low magnitude neurons hurts the model the least, and removing high magnitude ones cases the largest loss. This is a good argument that this criterion correlates well with neuron significance.\n\nBaseline recycling\nThe most straightforward reinitialization scheme is to sample the weights of the reinitialized neurons from the initial distribution. After examining the performance of this solution, we could not see any difference between recycling and vanilla training.\n\n\n\n\n                                                    \n\n\nAs a sanity check, we have examined the histogram presenting the number of times each neuron was recycled, discovering that the same small subset of neurons was being reinitialized over and over during training.\n\n\n\n\n                                                    \n\n\nAs we have seen , on average magnitude of neurons grows throughout the training. Therefore, sampling from the initial distribution will cause the reycycled neurons to have even lower magnitudes. As an effect, they are unable to catch up to before another selection phase. Thus, the recycled neurons are caught up in a vicious cycle in which they are always recycled before achieving high magnitude.\n\n\n\nImmunity\nTo address the problem we observed in the previous approach, we tried another strategy - recycling with immunity. The idea here is to encourage diverse recycling by making each recycled neuron immune to further recycling for some predefined number of steps. We hypothesized that a reinitialized neuron needs some time to grow, which was not possible in the initial setting. The following plot illustrates that immunity prevents the recycled neurons from being catched in a vicious cycle.\n\n\n\n\n                                                    \n\n\nHigher number of immunity rounds (i.e. number of selection phases when a newly recycled neuron can’t be chosen) causes more neurons to be reinitialized at least once. Unfortunately, this eventually causes well-behaving parts of the network to be chosen for recycling. As an effect, the performance drops.\n\n\n\n\n                                                    \n\n\n\n\nModifying reinitialization distribution\nAs we have pointed out before, magnitude and weight distribution drifts away from the initial distribution as the training progresses. However, during our initial attempts, we initialized the weights sampling from the initial distribution. To fix this issue, we decided to try out another weight sampling technique. In this approach we used the normal distribution with mean and standard deviation equal to the mean and standard deviation of all the weights in the respective matrix. This approach, like immunity, eliminated the vicious cycle problem.\n\n\n\n\n                                                    \n\n\nHowever, this process introduced a lot of noise with adverse effect on the model’s loss.\n\n\n\n\n                                                    \n\n\n\n\nCopying existing neurons\nAt this stage of our project, we gained interest in the topic of growing neural networks. Intuitively, this problem has a similar part, in which we want to add new neurons or weights. In the case of Large Language Models, the topic is mentioned in the Gopher paper. The authors describe that for their experiments, copying existing neurons was the best strategy. We have also tried it, but without positive results.\n\n\n\n\n                                                    \n\n\n\n\nSmooth recycling\nIn an effort to explain our past problems, we pointed out the discrete nature of our technique, as opposed to the continuous landscape of training/optimizers. To change this, we have modified our strategy to gradually change the value by linear interpolation. We have noticed that this way we were able to make training loss dynamics smoother, but without beating the baseline.\n\n\n\n\n                                                    \n\n\n\n\n\nTangent - Midpoint Loss\nWhile working towards the main goal of the project, we started investigating the distribution of neuron magnitudes during the training. We noticed that the magnitude distribution is quite uneven - a large percentage of neurons remains small, and the distribution is right-skewed. Since the goal of our project was to reduce the number of low-quality, i.e., small neurons, we came up with a pretty risky solution: Midpoint Loss. The idea was to introduce an additional loss term that would encourage neuron growth and “even-ness” of the magnitude distribution. The general equation for the midpoint loss is:\n\\[ Loss = \\sum_{l = 1}^{L} \\sum_{n = 1}^{d_\\text{ff}} dist(M_{l,n}, stop\\_grad(avg(M_{l,*})))\\] where:\n\n\\(M_{l,n}\\) - magnitude of th \\(n^{\\text{th}}\\) neuron in the \\(l^{\\text{th}}\\) layer. In some experiments we used the \\(log\\) of the magnitude.\n\\(dist\\) - distance function, typically \\(l_1\\), \\(l_2\\).\n\\(avg\\) - arithmetic mean. In some experiments, median was used instead due to its robustness to outliers.\n\\(L\\) - number of layers.\n\\(d_\\text{ff}\\) - number of neurons in a layer. In some experiments, we only summed over neurons with magnitude below the average magnitude of the layer, to encourage growth of small neurons.\n\\(stop\\_grad\\) - stops the gradient from flowing through. Without stopping the gradient, the loss gradient computation becomes untractable.\n\nSince this idea is quite similar to weight decay, we decided not to optimize this term with Adam, but to split it from the task loss and optimize it using simple gradient descent - a similar technique is used in AdamW to incorporate weight decay loss term.\n \nMidpoint loss achieved the goal of boosting the small neurons, however it failed to make a positive impact on the model’s performance.\n\n\n\n\n                                                    \n\n\n\n\n\nConclusion\nIn this work, we described our attempts to integrate pruning and Lottery Ticket Hypothesis via neuron recycling. Although we were not able to beat the baseline using our technique, we explored the topic and conducted a series of experiments. We hope that our findings may be a helpful resource for future studies and investigations in this area."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To Our Blog",
    "section": "",
    "text": "This is the first post in our blog.\nWelcome!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "llm-random",
    "section": "",
    "text": "Neuron Recycling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2023\n\n\nJakub Krajewski †, Maciej Pióro †, Sebastian Jaszczur, Marek Cygan\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To Our Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nMaciej Pióro\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/neuron_recycling/index.html#baseline-reinitialization",
    "href": "posts/neuron_recycling/index.html#baseline-reinitialization",
    "title": "Neuron Recycling",
    "section": "Baseline reinitialization",
    "text": "Baseline reinitialization\nThe most straightforward reinitialization scheme is to sample the weights of the reinitialized neurons from the initial distribution. After examining the performance of this solution, we could not see any difference between recycling and vanilla training. As a sanity check, we have examined the histogram presenting the number of times each neuron was recycled, discovering that the same small subset of neurons was being reinitialized over and over during training. As we have seen , on average magnitude of neurons grows throughout the training. Therefore, the newly sampled neurons have, on average, lower magnitudes than non-recycled neurons and are unable to catch up to non-recycled neurons before another selection phase. Thus, the recycled neurons are caught up in a vicious cycle in which they are always recycled before achieving high magnitude.\n\n&lt;spróbować zrobić diagram z vicious cycle&gt;\nAnother thing we tried to make reinitialization work was Immunity. The idea here is to encourage diverse recycling by making each recycled neuron immune to further recycling for some predefined number of steps. We hypothesized that a reinitialized neuron needs some time to grow, but in our initial setting, the neuron is recycled before enough growth happens, keeping the smaller neurons in a vicious cycle of periodically reoccurring reinitialization. Unfortunately, in our experiments, the reinitialized neurons failed to grow even if given immunity. At the same time immunity of the small neurons lead to recycling of the large neurons, hurting the model’s performance.\n&lt;graph in which initial recycling are not visible, but later once show it hurts performance&gt;\nAt this point, we hypothesized that coming back to the initial distribution may not be the optimal choice, as weights of the neurons evolve while they are trained. The next idea we wanted to experiment with was simply sampling from the distribution that occurred across the entire layer at any given moment. Our approach was straightforward: we randomly selected from all the weights in the entire matrix. Here again, we noted the impact of our technique, but it was only when we conducted such intensive recycling that performance deteriorated. ^ Tu opisać reinicjalizację z dystrybucją z momentu recyklingu\n\nAt this stage of our project, we gained interest in the topic of growing neural networks. Intuitively, this problem has a similar part, in which we want to add new neurons or weights. In the case of Large Language Models, the topic is mentioned in [Gopher]. The authors describe that for their experiments, copying existing neurons was the best strategy. We have also tried this strategy, but without positive results.\nStarając się wyjaśnić nasze dotychczasowe problemy, zwróciliśmy uwagę na dyskretną naturę naszej techniki, as opposed to the continuous landscape of training/optimizers. To change this, we have modified our strategy to gradually change the value by linear interpolation, according to the rule\n&lt;latex ze wzorkiem; może jakiś plot z naszymi górkami&gt;,\nwhere was frozen and trainable. We have noticed that this way we were able to make training loss dynamics smoother, but still without any positive effect.\nIn general, we couldn’t beat the baseline in terms of performance."
  },
  {
    "objectID": "posts/neuron_recycling/index.html#baseline-recycling",
    "href": "posts/neuron_recycling/index.html#baseline-recycling",
    "title": "Neuron Recycling",
    "section": "Baseline recycling",
    "text": "Baseline recycling\nThe most straightforward reinitialization scheme is to sample the weights of the reinitialized neurons from the initial distribution. After examining the performance of this solution, we could not see any difference between recycling and vanilla training. As a sanity check, we have examined the histogram presenting the number of times each neuron was recycled, discovering that the same small subset of neurons was being reinitialized over and over during training. As we have seen , on average magnitude of neurons grows throughout the training. Therefore, the newly sampled neurons have, on average, lower magnitudes than non-recycled neurons and are unable to catch up to non-recycled neurons before another selection phase. Thus, the recycled neurons are caught up in a vicious cycle in which they are always recycled before achieving high magnitude.\n\n\n\n\n\n\n                                                    \n\n\nFigure 1: Figure description\n\n\n\n\n\n\n                                                    \n\n\n&lt;spróbować zrobić diagram z vicious cycle&gt;"
  },
  {
    "objectID": "posts/neuron_recycling/index.html#section",
    "href": "posts/neuron_recycling/index.html#section",
    "title": "Neuron Recycling",
    "section": "",
    "text": "&lt;graph in which initial recycling are not visible, but later once show it hurts performance&gt;\nAt this point, we hypothesized that coming back to the initial distribution may not be the optimal choice, as weights of the neurons evolve while they are trained. The next idea we wanted to experiment with was simply sampling from the distribution that occurred across the entire layer at any given moment. Our approach was straightforward: we randomly selected from all the weights in the entire matrix. Here again, we noted the impact of our technique, but it was only when we conducted such intensive recycling that performance deteriorated. ^ Tu opisać reinicjalizację z dystrybucją z momentu recyklingu\n\nAt this stage of our project, we gained interest in the topic of growing neural networks. Intuitively, this problem has a similar part, in which we want to add new neurons or weights. In the case of Large Language Models, the topic is mentioned in [Gopher]. The authors describe that for their experiments, copying existing neurons was the best strategy. We have also tried this strategy, but without positive results.\nStarając się wyjaśnić nasze dotychczasowe problemy, zwróciliśmy uwagę na dyskretną naturę naszej techniki, as opposed to the continuous landscape of training/optimizers. To change this, we have modified our strategy to gradually change the value by linear interpolation, according to the rule\n&lt;latex ze wzorkiem; może jakiś plot z naszymi górkami&gt;,\nwhere was frozen and trainable. We have noticed that this way we were able to make training loss dynamics smoother, but still without any positive effect.\nIn general, we couldn’t beat the baseline in terms of performance."
  },
  {
    "objectID": "posts/neuron_recycling/index.html#immunity",
    "href": "posts/neuron_recycling/index.html#immunity",
    "title": "Neuron Recycling",
    "section": "Immunity",
    "text": "Immunity\nAnother thing we tried to make reinitialization work was Immunity. The idea here is to encourage diverse recycling by making each recycled neuron immune to further recycling for some predefined number of steps. We hypothesized that a reinitialized neuron needs some time to grow, but in our initial setting, the neuron is recycled before enough growth happens, keeping the smaller neurons in a vicious cycle of periodically reoccurring reinitialization. Unfortunately, in our experiments, the reinitialized neurons failed to grow even if given immunity. At the same time immunity of the small neurons lead to recycling of the large neurons, hurting the model’s performance."
  },
  {
    "objectID": "posts/neuron_recycling/index.html#modifying-reinitialization-distrin",
    "href": "posts/neuron_recycling/index.html#modifying-reinitialization-distrin",
    "title": "Neuron Recycling",
    "section": "Modifying reinitialization distrin",
    "text": "Modifying reinitialization distrin\n&lt;graph in which initial recycling are not visible, but later once show it hurts performance&gt;\nAt this point, we hypothesized that coming back to the initial distribution may not be the optimal choice, as weights of the neurons evolve while they are trained. The next idea we wanted to experiment with was simply sampling from the distribution that occurred across the entire layer at any given moment. Our approach was straightforward: we randomly selected from all the weights in the entire matrix. Here again, we noted the impact of our technique, but it was only when we conducted such intensive recycling that performance deteriorated. ^ Tu opisać reinicjalizację z dystrybucją z momentu recyklingu\n\nAt this stage of our project, we gained interest in the topic of growing neural networks. Intuitively, this problem has a similar part, in which we want to add new neurons or weights. In the case of Large Language Models, the topic is mentioned in [Gopher]. The authors describe that for their experiments, copying existing neurons was the best strategy. We have also tried this strategy, but without positive results.\nStarając się wyjaśnić nasze dotychczasowe problemy, zwróciliśmy uwagę na dyskretną naturę naszej techniki, as opposed to the continuous landscape of training/optimizers. To change this, we have modified our strategy to gradually change the value by linear interpolation, according to the rule\n&lt;latex ze wzorkiem; może jakiś plot z naszymi górkami&gt;,\nwhere was frozen and trainable. We have noticed that this way we were able to make training loss dynamics smoother, but still without any positive effect.\nIn general, we couldn’t beat the baseline in terms of performance."
  },
  {
    "objectID": "posts/neuron_recycling/index.html#modifying-reinitialization-weights-distribution",
    "href": "posts/neuron_recycling/index.html#modifying-reinitialization-weights-distribution",
    "title": "Neuron Recycling",
    "section": "Modifying reinitialization weights distribution",
    "text": "Modifying reinitialization weights distribution\nAs we have pointed out before, magnitude and weight distribution drifts away from the initial distribution as the training progresses. However, during our initial attempts, we initialized the weights sampling from the initial distribution. To fix this issue, we decided to try out another weight sampling technique. A feed-forward layer consists of two matrices. Each time a neuron is reinitialized, the corresponding row and column in the matrices are filled with new (reinitialized) weights. The new technique, when replacing the weights, used the normal distribution with mean and standard deviation equal to the mean and standard deviation of all the weights in the respective matrix. This approach, like immunity, eliminated the vicious cycle. However, this process introduced a lot of noise with adverse effect on the model’s loss."
  },
  {
    "objectID": "posts/neuron_recycling/index.html#inverse-weight",
    "href": "posts/neuron_recycling/index.html#inverse-weight",
    "title": "Neuron Recycling",
    "section": "Inverse Weight",
    "text": "Inverse Weight\nWhile working towards the main goal of the project, we started investigating the distribution of neuron magnitudes during the training [link do sekcji z blogposta]. We noticed that the magnitude distribution is quite uneven - a large percentage of neurons remains small, and the distribution is right-skewed. Since the goal of our project was to reduce the number of low-quality, i.e., small neurons, we came up with a pretty risky solution: inverse weight decay. The idea was to introduce an additional loss term that would encourage neuron growth. The general equation for that loss is:  Since this idea is quite similar to weight decay, we decided not to optimize this term with Adam, but to split it from the task loss and optimize it using simple gradient descent - a similar technique is used in AdamW to incorporate weight decay loss term. Since we weren;c\n&lt;plot, że iwd zmienia dystrybucję magnitude&gt; &lt;wariacje nt. Plotu ze strzałkami tłumaczące różne opcje w immunity&gt;"
  },
  {
    "objectID": "posts/neuron_recycling/index.html#midpoint-loss",
    "href": "posts/neuron_recycling/index.html#midpoint-loss",
    "title": "Neuron Recycling",
    "section": "Midpoint Loss",
    "text": "Midpoint Loss\nWhile working towards the main goal of the project, we started investigating the distribution of neuron magnitudes during the training [link do sekcji z blogposta]. We noticed that the magnitude distribution is quite uneven - a large percentage of neurons remains small, and the distribution is right-skewed. Since the goal of our project was to reduce the number of low-quality, i.e., small neurons, we came up with a pretty risky solution: Midpoint Loss. The idea was to introduce an additional loss term that would encourage neuron growth and ``even-ness’’ of the magnitude distribution. The general equation for the midpoint loss is:\n\\[ Loss = \\sum_{l = 1}^{L} \\sum_{n = 1}^{dff} dist(M_{l,n}, stop\\_grad(midpoint(M_{l,*}))), \\] where:\n\n\\(M_{l,n}\\) -magnitude of then-th neuron in the l-th layer \\(dist\\) - some distance function, typically\\(l_1\\),\\(l_2\\) \\(midpoint\\) -mean or median. Median can be used instead of mean because of itsrobustness with respect\n\nSince this idea is quite similar to weight decay, we decided not to optimize this term with Adam, but to split it from the task loss and optimize it using simple gradient descent - a similar technique is used in AdamW to incorporate weight decay loss term. Since we weren;c\n&lt;plot, że iwd zmienia dystrybucję magnitude&gt; &lt;wariacje nt. Plotu ze strzałkami tłumaczące różne opcje w immunity&gt;"
  },
  {
    "objectID": "posts/neuron_recycling/index.html#tangent---midpoint-loss",
    "href": "posts/neuron_recycling/index.html#tangent---midpoint-loss",
    "title": "Neuron Recycling",
    "section": "Tangent - Midpoint Loss",
    "text": "Tangent - Midpoint Loss\nWhile working towards the main goal of the project, we started investigating the distribution of neuron magnitudes during the training [link do sekcji z blogposta]. We noticed that the magnitude distribution is quite uneven - a large percentage of neurons remains small, and the distribution is right-skewed. Since the goal of our project was to reduce the number of low-quality, i.e., small neurons, we came up with a pretty risky solution: Midpoint Loss. The idea was to introduce an additional loss term that would encourage neuron growth and ``even-ness’’ of the magnitude distribution. The general equation for the midpoint loss is:\n\\[ Loss = \\sum_{l = 1}^{L} \\sum_{n = 1}^{dff} dist(M_{l,n}, stop\\_grad(midpoint(M_{l,*}))), \\] where:\n\n\\(M_{l,n}\\) -magnitude of then-th neuron in the l-th layer \\(dist\\) - some distance function, typically\\(l_1\\),\\(l_2\\) \\(midpoint\\) -mean or median. Median can be used instead of mean because of itsrobustness with respect\n\nSince this idea is quite similar to weight decay, we decided not to optimize this term with Adam, but to split it from the task loss and optimize it using simple gradient descent - a similar technique is used in AdamW to incorporate weight decay loss term. Since we weren;c\n&lt;plot, że iwd zmienia dystrybucję magnitude&gt; &lt;wariacje nt. Plotu ze strzałkami tłumaczące różne opcje w immunity&gt;"
  },
  {
    "objectID": "posts/neuron_recycling/index.html#copying-existing-neurons",
    "href": "posts/neuron_recycling/index.html#copying-existing-neurons",
    "title": "Neuron Recycling",
    "section": "Copying existing neurons",
    "text": "Copying existing neurons\nAt this stage of our project, we gained interest in the topic of growing neural networks. Intuitively, this problem has a similar part, in which we want to add new neurons or weights. In the case of Large Language Models, the topic is mentioned in [Gopher]. The authors describe that for their experiments, copying existing neurons was the best strategy. We have also tried it, but without positive results."
  },
  {
    "objectID": "posts/neuron_recycling/index.html#model-and-training-setup",
    "href": "posts/neuron_recycling/index.html#model-and-training-setup",
    "title": "Neuron Recycling",
    "section": "Model and training setup",
    "text": "Model and training setup\nIn our project, we are focusing on the Transformer , since it’s a major architecture across different domains . For the specific type of the model, we are working on encoder-only BERT . Taking into consideration available computational resources and expected iteration time (we wanted to try as many options as possible), we decided to opt for the BERT Medium configuration (with \\(d_\\text{model}=512\\) and \\(8\\) attention heads). We focus on the feed-forward layer, because it is the most computationally demanding part of commonly-used transformer models and, in large models, it contains the majority of the parameters. At the same time, the amount of research focusing on the attention mechanism is overwhelming, suggesting that the feed-forward layer is a relatively unexplored area.\nWe trained the model for \\(80{,}000\\) steps (around compute-optimal point), with Adam, using batch size of \\(256\\) and learning rate of \\(0.001\\).\nIt has been shown , that the initialization of the weights in neural networks is of utmost importance to the convergence of the network. In all our experiments, we initialized the linear layers using the following distribution:  as per .\nIn the following part of this post, we will often use the terms of neuron and its magnitude. Below are the definitions we employ.\n\nNeuron  Neurons are the basic building blocks of artificial neural networks. A neuron consists of some weights going “into” the neuron (\\(w_{in}\\text{'s}\\)), an activation function (we use ReLU), and the weights going “out of” the neuron (\\(w_{out}\\text{'s}\\)). Technically speaking, the neurons’ weights are the parameters in the corresponding column and row of the two feed-forward matrices.\nMagnitude  The magnitude of a neuron is given as \\(M=\\sqrt{\\sum{w_{in}^2} \\sum{w_{out}^2}}\\). The magnitude of a weight is simply its absolute value."
  },
  {
    "objectID": "posts/neuron_recycling/index.html#important-definitions",
    "href": "posts/neuron_recycling/index.html#important-definitions",
    "title": "Neuron Recycling",
    "section": "Important definitions",
    "text": "Important definitions\n\nNeuron  Neurons are the basic building blocks of artificial neural networks. A neuron consists of some weights going “into” the neuron (\\(w_{in}\\text{'s}\\)), an activation function (we use ReLU), and the weights going “out” of the neuron (\\(w_{out}\\text{'s}\\)). Technically speaking, the neurons’ weights are a column and a row of two matrices. &lt;coś o tym, że u nas zawsze są dwa bloki liniowe + napisać o macierzach&gt;\nMagnitude  The magnitude of a neuron is given as \\(M=\\sqrt{\\sum{w_{in}^2} \\sum{w_{out}^2}}\\). The magnitude of a weight is simply its absolute value."
  },
  {
    "objectID": "posts/neuron_recycling/index.html#pruning",
    "href": "posts/neuron_recycling/index.html#pruning",
    "title": "Neuron Recycling",
    "section": "Pruning",
    "text": "Pruning\nPruning is a technique used to induce sparsity and decrease the parameter count in a neural network. In simple terms, it means deleting the least important neurons (structured pruning) or weights (unstructured pruning). A typical implementation realizes this by either multiplying the output of the deleted neurons by 0 or setting the weights of the neuron to 0. A widely-used proxy for the importance of a neuron or weight is its magnitude. You may find it counterintuitive, but we can even remove the whole FF Layer and the model will still work. This is because FF can be approximated by Attention.\nIt’s important to note that the Transformer can be trained  totally without the feed-forward layer (with a loss in performance), since the model can learn to represent the same transformation using attention. Below we present a plot with loss curves of the model gradually pruned at the FF layer, starting in step \\(10{,}000\\), such that the layer is completely masked in the end of the training.\n\n\n\n\n                                                    \n\n\nAs you can see, the effect of pruning can’t be visible for a significant fraction of the training time. It’s also worth noting that in the end the model without FF Layer performs slightly better than the pruned one. This is because in the first case, Attention was trained to adjust from the very beginning of the training."
  },
  {
    "objectID": "posts/neuron_recycling/index.html#the-goal",
    "href": "posts/neuron_recycling/index.html#the-goal",
    "title": "Neuron Recycling",
    "section": "The goal",
    "text": "The goal\n\nThe end-goal of the project was to create a method that would allow us to make better use of the parameters in the feed-forward layer. In this context, a natural question arises - against what baseline should our results be compared? To answer this question, we trained the model with differing dimensionalities of the feed-forward layer. The results are presented below. The true BERT Medium configuration has \\(d_\\text{ff}=2048\\), and, as expected, the model’s performance drops when the \\(d_\\text{ff}\\) is decreased and increases when the \\(d_\\text{ff}\\) is increased."
  },
  {
    "objectID": "posts/neuron_recycling/index.html#smooth-recycling",
    "href": "posts/neuron_recycling/index.html#smooth-recycling",
    "title": "Neuron Recycling",
    "section": "Smooth recycling",
    "text": "Smooth recycling\nIn an effort to explain our past problems, we pointed out the discrete nature of our technology, as opposed to the continuous landscape of training/optimizers. To change this, we have modified our strategy to gradually change the value by linear interpolation. We have noticed that this way we were able to make training loss dynamics smoother, but without beating the baseline."
  }
]