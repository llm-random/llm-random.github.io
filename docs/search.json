[
  {
    "objectID": "posts/neuron_recycling/index.html",
    "href": "posts/neuron_recycling/index.html",
    "title": "Neuron Recycling",
    "section": "",
    "text": "Sparse neural networks have garnered attention due to their theoretical promise of lowered computational demands and memory savings. However, to this date, the theoretical gains have largely failed to materialize due to the lack of hardware support for this kind of models. In this work, we explore the idea of neuron recycling which is inspired by pruning - a method often employed to induce sparsity in neural networks. We also present lessons we have learned along the way.\n\nIntroduction\nPruning is a well-established technique used to sparsify neural networks (LeCun et al., 1989; Han et al., 2015). It relies on the fact that typically a large part of a trained neural network can be masked without impacting the accuracy of the network, albeit often requiring additional fine-tuning in order to regain some lost performance. Despite multiple works proposing various neuron-selection criteria for pruning, magnitude-based pruning remains a viable option. The Lottery Ticket Hypothesis (Frankle and Carbin, 2019) is a major finding on the way to explain how the initialization impacts neural networks. The main point of the LTH is that through iterative pruning, performant subnetworks depending on the initialization can be found in neural networks. Those well-initialized network fragments are the namesake of LTH (the “lottery tickets”). Although some notions of the original LTH paper have been challenged (Frankle et al., 2020), it has remained the subject of active research and a motivation for our work. \nBy combining the two ideas (pruning and LTH) we arrive at a new potential technique for raising neural network performance. If we are able to remove parts of the network without hurting the performance (pruning) and the fitness of a part of a network is determined at initialization, perhaps we could re-initialize the unnecessary network parts (i. e. draw more “lottery tickets”), leading to a better-performing network.\n\n\n\nPreliminaries\nBefore we move to the presentation of our experiments and findings, let’s first discuss the training setup, define key terminology, and go over the basics.\n\nModel and training setup\nIn our project, we are focusing on the Transformer (Vaswani et al., 2017), since it’s a major architecture across different domains (Touvron et al., 2023; Dosovitskiy et al., 2021). For the specific type of the model, we are working on encoder-only BERT (Devlin et al., 2019). Taking into consideration available computational resources and expected iteration time (we wanted to try as many options as possible), we decided to opt for the BERT Medium configuration (with \\(d_\\text{model}=512\\) and \\(8\\) attention heads). We focus on the feed-forward layer, because it is the most computationally demanding part of commonly-used transformer models and, in large models, it contains the majority of the parameters. At the same time, the amount of research focusing on the attention mechanism is overwhelming, suggesting that the feed-forward layer is a relatively unexplored area.\nWe trained the model for \\(80{,}000\\) steps (around compute-optimal number of train samples for this size of model) , with Adam (Kingma and Ba, 2017), using batch size of \\(256\\) and learning rate of \\(0.001\\). We used Kaiming uniform (He et al., 2015) initialization in the feed-forward layer. For the training objective, we use masked language modeling loss, as described in (Devlin et al., 2019).\nIn the following part of this post, we will often use the terms neuron and magnitude. Below are the definitions we employ.\nNeuron. In the Transformer, feed-forward layer consists of two linear layers, with a nonlinearity in between. The first layer maps the input vector from \\(d_\\text{model}\\) to \\(d_\\text{ff}\\) dimension, and the second one from \\(d_\\text{ff}\\) back to \\(d_\\text{model}\\). Typically, \\(d_\\text{ff}\\) is four times greater than \\(d_\\text{model}\\). By neuron, we will understand all weights interacting with a particular coordinate in the \\(\\mathbb{R}^{d_\\text{ff}}\\) activation vector. In the torch implementation, a neuron’s weights are the parameters in a given row of the first feed-forward matrix and in the corresponding column in the second one. \nMagnitude. To calculate magnitude of a weight, we will use its absolute value. As the magnitude of the \\(i\\)-th neuron we will use value of the expression \\[M= \\lVert x_i^{in}\\rVert \\cdot \\lVert x_i^{out}\\rVert,\\] where:\n\n\\(\\lVert x_i^{in}\\rVert\\) - \\(l_2\\) norm of the \\(i\\)-th row in the weight matrix of the input linear layer\n\\(\\lVert x_i^{out}\\rVert\\) - \\(l_2\\) norm of the \\(i\\)-th column in the weight matrix of the output linear layer.\n\n\n\nPruning\nPruning is a technique used to induce sparsity and decrease the parameter count in a neural network. In simple terms, it means deleting the least important neurons (structured pruning) or weights (unstructured pruning). A typical implementation realizes this by either multiplying the output of the deleted neurons by 0 or setting the weights of the neuron to 0. A widely-used proxy for the importance of a neuron or weight is its magnitude. Notably, the network can still be trained even if the architecture doesn’t contain feed-forward layer, because the model can learn to represent the same trainsformation using only Attention. However, without FF the training time needed to achieve the same performance is much longer.\n\nBelow we present a plot with loss curves of the model gradually pruned at the FF layer, starting in step \\(10{,}000\\), such that the layer is completely masked in the end of the training. In this case, we perform structured pruning, i.e. we mask the whole neurons. As a comparison, we also add regular model and the one without feed-forward layer.\n\n\n\n\n                                                    \n\n\nInterestingly, the effect of pruning can’t be visible for a significant fraction of the training time. It’s also worth noting that in the end the model without FF Layer performs slightly better than the pruned one. This is because in the first case, Attention was trained to adjust  from the very beginning of the training.\n\n\nThe goal\n\n\nThe end-goal of the project was to create a method that would allow us to make better use of the parameters in the feed-forward layer. In this context, a natural question arises - against what baseline should our results be compared? To answer this question, we trained the model with differing dimensionalities of the feed-forward layer. The results are presented below.\n\n\n\n\n                                                    \n\n\nThe true BERT Medium configuration has \\(d_\\text{ff}=2048\\). As we might expect, the performance drops when \\(d_\\text{ff}\\) is decreased and improves when \\(d_\\text{ff}\\) is increased. In particular, the model with the feed-forward layer two times wider than the baseline achives the same loss in approximately 20% fewer steps. This shows the direction for our project: through neuron recycling, we want the model to behave more like the one with larger \\(d_\\text{ff}\\) by making a better use of available parameters.\n\n\n\nUnderstanding neuron magnitudes\nOne of the key inspirations for our work was structured pruning, where neuron/filter magnitude is often chosen as the measure of significance (Li et al., 2017; He et al., 2018). We were interested in how this metric evolves during the training process. At first, we thought a histogram of neuron magnitudes would exhibit a normal distribution. However, our experiments showed something different. The following graph shows evolution of neuron magnitudes throughout the training process. \n\n\n\n\n    \n\n\n\n    \n        \n        \n    \n\n\n\n\nIn the early stages of training, the neurons split into two groups, one with much lower magnitudes than the other. This finding opens up many discussion topics. One could guess that the neurons belonging to the group with smaller magnitudes potentially don’t hold much importance and can be pruned freely. However, it’s also possible that these neurons, though small, play a critical role in specific tasks.\nThis phenomenon is not limited to the first layer of the network. We have observed it in all layers, apart from the last one, as shown in the following plot.\n\n\n\n\n    \n\n\n\n    \n        \n        \n    \n\n\n\n After examining these experiments, we were trying to understand why in the early layers we observed two distinct groups of neurons, categorized by their magnitudes. One possible explanation is that certain parts of the network receive a smaller signal and are slower to improve in training. We designed an experiment to check that. We periodically froze all parts of the network except for the feed-forward component and continued to train it for several batches of data. We hypothesized that in this scenario, weaker neurons might catch up, resulting in a more even distribution. We called this procedure overtraining  feed-forward layer. It’s important to note that this approach is impractical and computationally heavy, but we wanted to use it for the purpose of illustration. The results are depicted in the following plot.\n\n\n\n\n                                                    \n\n\n\n\nWe can see that the group of weaker  neurons has moved to the right after performing additional training of the FF part. However, neurons still form two distinct groups: overtraining the whole layer is not enough for the weaker ones to catch up. In the next experiment, we have examined the scenario of retraining only small magnitude neurons, only large magnitude neurons and random subsets. How does it affect the performance? The results are depicted on the following plot.\n\n\n\n\n\n                                                    \n\n\n\nOvertraining only the smallest neurons yields the best results when compared to reinforcing high-magnitude ones. Notably, overtraining the small ones gives similar gains in performance to working on the entire layer! Contrarily, ampifying the highest ones gives gains comparable to no overtraining at all. This provides a compelling argument in favor of our technique, suggesting that we can achieve significant gain by improving the the functionality of low-magnitude neurons. \n\nMagnitudes in openly available pretrained models\n\nSo far, we have performed a series of experiments in one particular setting. We were curious to see how our observations would translate to well-established, large-scale foundation models like BERT Large or T5.\n\n\n\n\n                                                    \n\n\n\n\n\n                                                    \n\n\nThere is a clear difference between the plots above. Magnitudes in T5 seem similar to those in our smaller models, while BERT Large presents a more balanced distribution. What could account for these variations? It turns out that an important difference between the training process of these models was the use of weight decay. Intuitively, adding a component to the loss function that corresponds to the magnitudes of weights, helps balance the magnitudes of neurons. In the following part of the article we will further explore the idea of explicitly changing the magnitude distribution through modification of the loss function. \nFindings presented above support the idea of exploring neuron recycling and offer a good foundation for further experiments. In the next sections, we will present results on this topic and share our insights.\n\n\n\nRecycling\n\nThe central part of our work was a method we called neuron recycling. The process consists of three phases, repeated periodically: training, selection and reinitialization.\n\n\n\n\n\n\nIn the training phase, the model is trained to predict masked tokens (masked language modelling).\nIn the selection phase, the least important neurons are determined, where the baseline criterion is neuron magnitude.\nIn the reinitialization phase, new weights are assigned to neurons.\n\nAlthough this procedure is conceptually simple, it allows for many degrees of freedom. Here are some choices that can be made:\n\nThe number of training steps before consecutive selection / reinitialization phases\nThe percentage of recycled neurons\nSelection / reinitialization strategies\n\nAfter examining the pruning literature, we found that the simple magnitude-based approach works well in most cases (Blalock et al., 2020; Maene et al., 2021). Moreover, it is easy to implement and computationally efficient. This approach is also grounded in our experiments. Below we present the training curves for the model pruned gradually using different criterions: high/low magnitude and random neurons.\n\n\n\n\n                                                    \n\n\n\n\nAs you can see, removing low magnitude neurons hurts the model the least, and removing high magnitude ones cases the largest loss. This is a good argument that this criterion correlates well with neuron significance.\n\nBaseline recycling\nThe most straightforward reinitialization scheme is to sample the weights of the reinitialized neurons from the initial distribution. After examining the performance of this solution, we could not see any difference between recycling and vanilla training.\n\n\n\n\n                                                    \n\n\nAs a sanity check, we have examined the histogram presenting the number of times each neuron was recycled, discovering that the same small subset of neurons was being reinitialized over and over during training.\n\n\n\n\n                                                    \n\n\nAs we have seen in the previous section, on average magnitude of neurons grows throughout the training. Therefore, sampling from the initial distribution will cause the reycycled neurons to have even lower magnitudes. As an effect, they are unable to catch up to before another selection phase. Thus, the recycled neurons are caught up in a vicious cycle in which they are always recycled before achieving high magnitude.\n\n\n\nImmunity\nTo address the problem we observed in the previous approach, we tried another strategy - recycling with immunity. The idea here is to encourage diverse recycling by making each recycled neuron immune to further recycling for some predefined number of steps. We hypothesized that a reinitialized neuron needs some time to grow, which was not possible in the initial setting. The following plot illustrates that immunity prevents the recycled neurons from being catched in a vicious cycle.\n\n\n\n\n                                                    \n\n\nHigher number of immunity rounds (i.e. number of selection phases when a newly recycled neuron can’t be chosen) causes more neurons to be reinitialized at least once. Unfortunately, this eventually causes well-behaving parts of the network to be chosen for recycling. As an effect, the performance drops.\n\n\n\n\n                                                    \n\n\n\n\nModifying reinitialization distribution\nAs we have pointed out before, magnitude and weight distribution drifts away from the initial distribution as the training progresses. However, during our initial attempts, we initialized the weights sampling from the initial distribution. To fix this issue, we decided to try out another weight sampling technique. In this approach we used the normal distribution with mean and standard deviation equal to the mean and standard deviation of all the weights in the respective layer. This approach, like immunity, eliminated the vicious cycle  problem.\n\n\n\n\n                                                    \n\n\nHowever, this process introduced a lot of noise with adverse effect on the model’s loss.\n\n\n\n\n                                                    \n\n\n\n\nCopying existing neurons\nIn the problem of growing  or warm starting  neural networks, the aim is to gradually add new weights to the model througout the training. In the case of Large Language Models, this topic is mentioned in the Gopher (Rae et al., 2022) paper. In particular, the authors describe multiple strategies for adding new neurons to the feed-forward layer and conclude that copying existing ones (with an addition of small noise) seems to give the best results. We tried this approach in our setting, but couldn’t observe better performance.\n\n\n\n\n                                                    \n\n\n\n\nSmooth recycling\nWe came up with the hypothesis that neuron recycling could actually work better if it didn’t have sudden and discrete changes in neuron values. These sharp changes plausibly destabilize the training process. This issue is clear in sudden loss spikes, such as those observed in the recycling with modified distribution part. It may be particularly problematic that the statistics of the optimizer need to adjust to the new values, but they don’t have time to do that. To make the recycling process smoother, we modified our strategy to linearly interpolate between the old weights of the neuron and the their target values. More precisely, the new value assigned for a recycled weight in this approach is \\[ x = \\alpha \\ x_{target} + (1-\\alpha) \\ x_{old},\\] where:\n\n\\(x_{target}\\) - target value chosen for the weight; this parameter is trainable right away\n\\(x_{old}\\) - old value of the weight before recycling; this value is no longer trainable\n\\(\\alpha\\) - non-trainable parameter, changed linearly from 0 to 1 over 1000 steps following the selection phase.\n\nWith this modification, we saw that the training loss became smoother. However, the solution was still not able to beat the baseline. \n\n\n\n\n                                                    \n\n\n\n\n\nTangent - Midpoint Loss\nWhile inspecting the distribution of neuron magnitudes during the training, we can notice that it is quite uneven - a large percentage of neurons remains small, and the distribution is right-skewed. Since the goal of our project was to reduce the number of low-quality, i.e., small neurons, we came up with a pretty risky solution: Midpoint Loss. The idea was to introduce an additional loss term that would encourage neuron growth and “even-ness” of the magnitude distribution. The general equation for the midpoint loss is\n\\[ Loss = \\sum_{l = 1}^{L} \\sum_{n = 1}^{d_\\text{ff}} \\ d\\left( M_{l,n}, \\ sg\\left(\\bar{M}_{l}\\right) \\right)\\] where:\n\n\\(M_{l,n}\\) - magnitude of th \\(n^{\\text{th}}\\) neuron in the \\(l^{\\text{th}}\\) layer. In some experiments we used the \\(log\\) of the magnitude\n\\(\\bar{M}_{l}\\) - average neuron magnitude in layer \\(l\\), typically calculated as arithmetic mean. In some experiments, median was used instead due to its robustness to outliers\n\\(sg\\) - stops the gradient from flowing through\n\\(d\\) - distance function, typically \\(l_1\\) or \\(l_2\\)\n\\(d_\\text{ff}\\) - number of neurons in a layer. In some experiments, we only summed over neurons with magnitude below the average magnitude of the layer, to encourage growth of small neurons, without thwarting the growth of the large ones\n\\(L\\) - number of layers.\n\nSince this idea is quite similar to weight decay, we decided not to optimize this term with Adam, but to split it from the task loss and optimize it using simple gradient descent - a similar technique is used in AdamW (Loshchilov and Hutter, 2019) to incorporate weight decay loss term.\n\n\n\n\n                                                    \n\n\n\nMidpoint loss achieved the goal of boosting the small neurons, however it failed to make a positive impact on the model’s performance.\n\n\n\n\n                                                    \n\n\n\n\n\n\nConclusion\nIn this work, we described our attempts to integrate pruning and Lottery Ticket Hypothesis via neuron recycling. Although we were not able to beat the baseline using our technique, we explored the topic thoroughly and conducted a series of experiments, providing valuable insights into the inner workings of a transformer. We hope that our findings may be a helpful resource for future studies and investigations in this area.   \n\n\nReferences\n\n\nDavis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. 2020. What is the state of neural network pruning?\n\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding.\n\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale.\n\n\nJonathan Frankle and Michael Carbin. 2019. The lottery ticket hypothesis: Finding sparse, trainable neural networks.\n\n\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. 2020. Linear mode connectivity and the lottery ticket hypothesis. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th international conference on machine learning, volume 119, pages 3259–3269. PMLR.\n\n\nSong Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights and connections for efficient neural network. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in neural information processing systems, volume 28. Curran Associates, Inc.\n\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification.\n\n\nYang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. 2018. Soft filter pruning for accelerating deep convolutional neural networks.\n\n\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A method for stochastic optimization.\n\n\nYann LeCun, John Denker, and Sara Solla. 1989. Optimal brain damage. In D. Touretzky, editor, Advances in neural information processing systems, volume 2. Morgan-Kaufmann.\n\n\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. 2017. Pruning filters for efficient ConvNets.\n\n\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization.\n\n\nJaron Maene, Mingxiao Li, and Marie-Francine Moens. 2021. Towards understanding iterative magnitude pruning: Why lottery tickets win.\n\n\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, et al. 2022. Scaling language models: Methods, analysis & insights from training gopher.\n\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and efficient foundation language models.\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need."
  },
  {
    "objectID": "posts/neuron_recycling/index.html#magnitudes-in-openly-available-pretrained-models",
    "href": "posts/neuron_recycling/index.html#magnitudes-in-openly-available-pretrained-models",
    "title": "Neuron Recycling",
    "section": "Magnitudes in openly available pretrained models",
    "text": "Magnitudes in openly available pretrained models\n\nSo far, we have performed a series of experiments on relatively small architectures. We were curious to see how our observations would translate to well-established, large-scale foundation models like BERT Large or T5.\n\n\n\n\n                                                    \n\n\n\n\n\n                                                    \n\n\nThere is a clear difference between the plots above. Magnitudes in T5 seem similar to those in our smaller models, while BERT presents a more balanced distribution. What could account for these variations? We discovered that the use of weight decay in BERT Large plays a significant role. This simple but widely used technique has an important impact on the distribution. \nThese findings support the idea of exploring neuron recycling and offer a good foundation for further experiments. In the next sections, we will present results on this topic and share our insights."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "llm-random",
    "section": "",
    "text": "RAW HTML CONTENT\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMixture of Tokens\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2023\n\n\nSzymon Antoniak *, Michał Krutul, Maciej Pióro, Jakub Krajewski, Jan Ludziejewski, Tomasz Odrzygóźdź, Marek Cygan ‡, Sebastian Jaszczur †\n\n\n\n\n\n\n  \n\n\n\n\nNeuron Recycling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nJakub Krajewski *, Maciej Pióro *, Sebastian Jaszczur †, Marek Cygan ‡\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "get-started.html",
    "href": "get-started.html",
    "title": "llm-random.github.io",
    "section": "",
    "text": "Install quarto: https://quarto.org/docs/get-started/\nVoilà\n\n\n\n\n\n$ quarto preview\n\n\n\n\n\n$ quarto render\n$ git commit -m \"Add new post\"\n$ git push"
  },
  {
    "objectID": "get-started.html#setup",
    "href": "get-started.html#setup",
    "title": "llm-random.github.io",
    "section": "",
    "text": "Install quarto: https://quarto.org/docs/get-started/\nVoilà"
  },
  {
    "objectID": "get-started.html#run-locally-hot-reload",
    "href": "get-started.html#run-locally-hot-reload",
    "title": "llm-random.github.io",
    "section": "",
    "text": "$ quarto preview"
  },
  {
    "objectID": "get-started.html#deployment",
    "href": "get-started.html#deployment",
    "title": "llm-random.github.io",
    "section": "",
    "text": "$ quarto render\n$ git commit -m \"Add new post\"\n$ git push"
  },
  {
    "objectID": "posts/cont_moe/index.html#moe-sa",
    "href": "posts/cont_moe/index.html#moe-sa",
    "title": "Mixture of Tokens",
    "section": "MoE SA",
    "text": "MoE SA\nExisting Mixture of Experts approaches have demonstrated very impressive results in training huge models for language [Switch], vision [sth by Basil] and more [sth by Basil again]. The core idea is to replace the FeedForward layer standard for Transformer architectures with a set of experts, usually also MLPs, and have each token be processed by a subset of experts. Their ability to drastically increase model size while keeping computational cost during training and inference constant has been shown to behave predictably with respect to parameter count and inference compute [scaling moe], similarly to the standard dense Transformer models [scaling,chinchilla]. Following the success of the initial huge parameter-count models, thorough analysis of the model training and inference schemes uncovered issues\n\n\ngeneric_moe"
  },
  {
    "objectID": "posts/cont_moe/index.html#limitations-of-current-approaches-sekcja-8.2-w-paperze-sa",
    "href": "posts/cont_moe/index.html#limitations-of-current-approaches-sekcja-8.2-w-paperze-sa",
    "title": "RAW HTML CONTENT",
    "section": "Limitations of Current Approaches (sekcja 8.2 w paperze) SA",
    "text": "Limitations of Current Approaches (sekcja 8.2 w paperze) SA\n\nWithout mentioning papers - obecne podejścia są niestabilne - mamy idealny balancing token -&gt; expert\nwith training stability, management of expert load (how many tokens are routed to each expert) that lead to time overheads, mode collapse (all tokens routed to a small portion of the expert set), leaving some tokens unprocessed, problems with expert exploration and specialization/redundancy, low performance of discrete operations on accelerators and finally communication costs in parallel training."
  },
  {
    "objectID": "posts/cont_moe/index.html#encoder-vs-decoder",
    "href": "posts/cont_moe/index.html#encoder-vs-decoder",
    "title": "RAW HTML CONTENT",
    "section": "Encoder vs Decoder",
    "text": "Encoder vs Decoder\nIn encoder-only models, each token may attend to the whole input sequence, whereas decoder-only models only allow attending to the tokens that came before a particular token to simulate the autoregressive decoding used during model inference. This is crucial for the training of decoder-only models, as allowing a token to attend to future tokens would constitute a data leak, preventing successful training. A major problem potentially facing any token-mixing strategy is the possibility of a data leak, making the strategy inappropriate for use with decoder-only models. In MoT, the tokens are mixed across batch, i.e. with other tokens at the same position in other samples in the same training batch. We don’t consider other strategies which would be more suitable in the context of encoders."
  },
  {
    "objectID": "posts/cont_moe/index.html#experimental-setup",
    "href": "posts/cont_moe/index.html#experimental-setup",
    "title": "RAW HTML CONTENT",
    "section": "Experimental setup",
    "text": "Experimental setup\nIn our experiments we train the models for \\(250{,}000\\) steps using a cosine LR scheduler with \\(2500\\) steps of warmup, and final LR equal to \\(0.1\\) of the peak LR. The context length is \\(256\\) tokens and the batch size is \\(256\\). We tune the max LR separately for each model."
  },
  {
    "objectID": "posts/cont_moe/index.html#co-tam-dalej",
    "href": "posts/cont_moe/index.html#co-tam-dalej",
    "title": "RAW HTML CONTENT",
    "section": "Co tam dalej",
    "text": "Co tam dalej\nContMoE vs dense vs expert choice (GPT-BERT mini) Each run was optimized for that many batches of given size Contmoe is better than dense"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Blog of LLM-Random research group, centered in IDEAS NCBR, started in mid-2022 by Sebastian Jaszczur. Members, in the order of joining, listed below.\n\nSebastian Jaszczur\nMarek Cygan (as advisor)\nJakub Krajewski\nSzymon Antoniak\nMaciej Pióro\nTomasz Odrzygóźdź\nJan Ludziejewski\nMichał Krutul\n\nPublic repositories of the group are available at LLM-Random organization on GitHub."
  },
  {
    "objectID": "posts/cont_moe/index.html",
    "href": "posts/cont_moe/index.html",
    "title": "RAW HTML CONTENT",
    "section": "",
    "text": "Mixture of Experts architectures have recently garnerned considerable attention for their ability to increase the size of Transformer architectures while keeping the computational cost of training and inference constant. The most successful MoE approaches achieve this by activating only subsets of a very large feed forward layer for each processed token.\nThis technique comes at a cost, though: the choice of that subset of parameters for a given token is a discrete operation, and as such its training requires either the use of gradient estimators for traditional gradient-based optimisation or non-gradient based techniques; the models are known to suffer from issues such as training instability, under- and overload of experts (subsets of the FeedForward layer) and more. While some of those problems can be alleviated with e.g. the use of various auxiliary losses or reduced initialisation scale, it is certain that existing MoE techniques are more difficult and less intuitive to train.\nWith this motivation, we propose Mixture of Tokens: a new, fully-differentiable type of architecture that retains all efficiency benefits of MoE and alleviates the aformentioned problem by mixing tokens before feeding them into the FeedForward layer. This technique is fully compatible with both masked and causual LLM training."
  },
  {
    "objectID": "posts/cont_moe/index.html#temeprature-annealing",
    "href": "posts/cont_moe/index.html#temeprature-annealing",
    "title": "Continuous Mixture-of-Experts",
    "section": "Temeprature annealing:",
    "text": "Temeprature annealing:\nIn base Continues-MoE setup tokens from various samples in the batch are mingled. This theoretically opens doors for being able to tease out the output of one batch sample by another. However, this may pave the way for potential issues.\nTo tackle this problem, we plan to conduct an experiment where in the final phase of this experiment will involve a strategic, gradual reduction of temperature, hile simultaneously maintaining a low loss - a process referred to as ‘temerature annealing’. The anticipated outcome is a discrete-routing model which promotes inference parallelization - a move expected to significantly boost the overall efficiency of our processes\n\nwygaszanie temperatury\nwiększe modele\nBERT (oczywiście, że będzie działać)"
  },
  {
    "objectID": "posts/cont_moe/index.html#temperature-annealing",
    "href": "posts/cont_moe/index.html#temperature-annealing",
    "title": "RAW HTML CONTENT",
    "section": "Temperature annealing:",
    "text": "Temperature annealing:\nIn base MoT setup tokens from various samples in the batch are mingled. This theoretically opens doors for being able to tease out the output of one batch sample by another. However, this may pave the way for potential issues.\nTo tackle this problem, we plan to conduct an experiment where in the final phase of this experiment will involve a strategic, gradual reduction of temperature, hile simultaneously maintaining a low loss - a process referred to as ‘temerature annealing’. The anticipated outcome is a discrete-routing model which promotes inference parallelization - a move expected to significantly boost the overall efficiency of our processes"
  },
  {
    "objectID": "posts/cont_moe/index.html#scaling-up-models",
    "href": "posts/cont_moe/index.html#scaling-up-models",
    "title": "RAW HTML CONTENT",
    "section": "Scaling Up Models",
    "text": "Scaling Up Models\nIn a previous section  we showcased results for a BERT-mini FLOP matched Continues-MoE. Our preliminary experiments are also suggesting promising outcomes for larger models. In upcoming weeks we aim to prepare comprehensive comparison of all associated models."
  },
  {
    "objectID": "posts/cont_moe/index.html#do-not-forget-about-a-bert",
    "href": "posts/cont_moe/index.html#do-not-forget-about-a-bert",
    "title": "RAW HTML CONTENT",
    "section": "Do not forget about a BERT",
    "text": "Do not forget about a BERT\nAs our results in this post covers only GPT model in future work we plan to compare resuts of Continues-MoE with other models on BERT. Our preliminary experiments are also suggesting promising outcomes for larger models. hus, we’re gearing up to provide a thorough comparison of all associated models in upcoming weeks."
  },
  {
    "objectID": "posts/cont_moe/index.html#implementation",
    "href": "posts/cont_moe/index.html#implementation",
    "title": "RAW HTML CONTENT",
    "section": "Implementation",
    "text": "Implementation\n\nIn the first phase of our MoT Layer, we partition batch sequences into groups of tokens, ensuring each group is of equal size.\nFor each group, we acquire a set of weights, named merge and emit. The sum of weights in both merge and emit equals 1 for every group, signifying the percentage of each token to utilize.\nIn every group, using merge weights, we consolidate each token group into a single token. Following this, we process all merged tokens with linear experts’ weights and a relu activation function. Employing emit weights, we redistribute resultant tokens from the experts’ layer back to a group of tokens.\nLastly, we disassemble the groups to align with the MoT input dimensions. - encoder vs decoder MP"
  },
  {
    "objectID": "posts/cont_moe/index.html#moe",
    "href": "posts/cont_moe/index.html#moe",
    "title": "Mixture of Tokens",
    "section": "MoE",
    "text": "MoE\nExisting Mixture of Experts approaches have demonstrated very impressive results in training huge models for language [Switch], vision [sth by Basil] and more [sth by Basil again]. The core idea is to replace the FeedForward layer standard for Transformer architectures with a set of experts, usually also MLPs, and have each token be processed by a subset of experts.\n\n\ngeneric_moe\n\n\n\n\n\n\nTheir ability to drastically increase model size while keeping computational cost during training and inference constant does lead to significant performance improvements, and has been shown to behave predictably with respect to parameter count and inference compute [scaling moe], similarly to the standard dense Transformer models [scaling,chinchilla].\nHowever, following the success of the initial huge parameter-count models, thorough analysis of the model training and inference schemes uncovered issues"
  },
  {
    "objectID": "posts/cont_moe/index.html#scaling-model-size-with-mixture-of-experts",
    "href": "posts/cont_moe/index.html#scaling-model-size-with-mixture-of-experts",
    "title": "RAW HTML CONTENT",
    "section": "Scaling model size with Mixture of Experts",
    "text": "Scaling model size with Mixture of Experts\nExisting Mixture of Experts approaches have demonstrated very impressive results in training huge models for language (Switch?), vision (moe_vision?) and more (moe_multimodal?). The core idea is to replace the FeedForward layer standard for Transformer architectures with a set of experts, usually also MLPs, and have each token be processed by a subset of experts.\n\n\ngeneric_moe\n\n\n\n\n\n\nThe increased model size, despite keeping computational cost during training and inference constant, leads to significant performance improvements, and has been shown to behave predictably with respect to parameter count and inference compute [scaling moe], similarly to the standard dense Transformer models [scaling,chinchilla].\nHowever, following the success of those initial huge parameter-count models, thorough analysis of the model training and inference schemes uncovered that using Mixture of Experts techiques brings about an entirely new suite of challenges."
  },
  {
    "objectID": "posts/cont_moe/plot1sa.html",
    "href": "posts/cont_moe/plot1sa.html",
    "title": "llm-random",
    "section": "",
    "text": "generic_moe"
  },
  {
    "objectID": "posts/cont_moe/diagrams/general1.html",
    "href": "posts/cont_moe/diagrams/general1.html",
    "title": "RAW HTML CONTENT",
    "section": "",
    "text": "&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;llm-random&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;llm-random&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\"&gt;About&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/about.html\"&gt;/about.html&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;llm-random - RAW HTML CONTENT&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;llm-random - RAW HTML CONTENT&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;llm-random - RAW HTML CONTENT&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;llm-random&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/mixture_of_tokens/index.html",
    "href": "posts/mixture_of_tokens/index.html",
    "title": "RAW HTML CONTENT",
    "section": "",
    "text": "new textold text\nnew textold text\nnew textold text\nnew textold text\nnew textold text\nnew textold text\nnew textold text\nnew textold text"
  },
  {
    "objectID": "posts/mixture_of_tokens/index.html#scaling-model-size-with-mixture-of-experts",
    "href": "posts/mixture_of_tokens/index.html#scaling-model-size-with-mixture-of-experts",
    "title": "RAW HTML CONTENT",
    "section": "Scaling model size with Mixture of Experts",
    "text": "Scaling model size with Mixture of Experts\nExisting Mixture of Experts approaches have demonstrated very impressive results in training huge models for language (Switch?), vision (moe_vision?) and more (moe_multimodal?). The core idea is to replace the FeedForward layer standard for Transformer architectures with a set of experts, usually also MLPs, and have each token be processed by a subset of experts.\n\n\ngeneric_moe\n\n\n\n\n\n\nThe increased model size, despite keeping computational cost during training and inference constant, leads to significant performance improvements, and has been shown to behave predictably with respect to parameter count and inference compute [scaling moe], similarly to the standard dense Transformer models [scaling,chinchilla].\nHowever, following the success of those initial huge parameter-count models, thorough analysis of the model training and inference schemes uncovered that using Mixture of Experts techiques brings about an entirely new suite of challenges."
  },
  {
    "objectID": "posts/mixture_of_tokens/index.html#limitations-of-current-approaches-sekcja-8.2-w-paperze-sa",
    "href": "posts/mixture_of_tokens/index.html#limitations-of-current-approaches-sekcja-8.2-w-paperze-sa",
    "title": "RAW HTML CONTENT",
    "section": "Limitations of Current Approaches (sekcja 8.2 w paperze) SA",
    "text": "Limitations of Current Approaches (sekcja 8.2 w paperze) SA\n\nWithout mentioning papers - obecne podejścia są niestabilne - mamy idealny balancing token -&gt; expert\nwith training stability, management of expert load (how many tokens are routed to each expert) that lead to time overheads, mode collapse (all tokens routed to a small portion of the expert set), leaving some tokens unprocessed, problems with expert exploration and specialization/redundancy, low performance of discrete operations on accelerators and finally communication costs in parallel training."
  },
  {
    "objectID": "posts/mixture_of_tokens/diagrams/general1.html",
    "href": "posts/mixture_of_tokens/diagrams/general1.html",
    "title": "RAW HTML CONTENT",
    "section": "",
    "text": "&lt;\\iframe&gt;\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;llm-random&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;llm-random&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\"&gt;About&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/about.html\"&gt;/about.html&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;llm-random - RAW HTML CONTENT&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;llm-random - RAW HTML CONTENT&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;llm-random - RAW HTML CONTENT&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;llm-random&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/mixture_of_tokens/index.html#mixture-of-experts-a-quick-recap",
    "href": "posts/mixture_of_tokens/index.html#mixture-of-experts-a-quick-recap",
    "title": "RAW HTML CONTENT",
    "section": "Mixture of Experts: A Quick Recap",
    "text": "Mixture of Experts: A Quick Recap\nIn recent years, there has been a growing interest in methods of increasing the number of parameters in neural networks while maintaining their computational demands. A major advance in eliminating some problems with traditional MoE approaches is Expert Choice (here biblio). Our work has been inspired by their approach - MoT can be seen as an Expert Choice (as opposed to Token Choice) kind of method.\nThere is also concurrent work (Softmoe) investigating ideas similar to ours in the vision domain. It is, however, limited to encoder-only models. It is worth noting that earlier, other fully-differentiable MoE approaches had been proposed (biblio copied from Softmoe). /* jakoś inaczej sformułowac: However, expert-mixing strategies presented in those works incur significant computational overhead. Also, they are token choice instead of expert choice, so different than us. */"
  },
  {
    "objectID": "posts/mixture_of_tokens/index.html#scaling-with-mixture-of-experts",
    "href": "posts/mixture_of_tokens/index.html#scaling-with-mixture-of-experts",
    "title": "RAW HTML CONTENT",
    "section": "Scaling with Mixture of Experts",
    "text": "Scaling with Mixture of Experts\nExisting (MoE)Mixture of Experts approaches have demonstrated very impressive results in training huge models for language (Switch?), vision (moe_vision?) and more (moe_multimodal?). The core idea is to replace the standard FeedForward layer from the FeedForward layer standard for Transformer architectures with a set of experts, usually also MLPs, and have each token to be processed by a subset of experts.\n\n\ngeneric_moe\n\n\n\n\n\n\nThe increased model size, despite keeping computational cost during training and inference constant, leads to significant performance improvements, and has been shown to behave predictably with respect to parameter count and inference compute [scaling moe], similarly to the standard dense Transformer models [scaling,chinchilla].\nHowever, following the success of those initial huge parameter-count models, thorough analysis of the model training and inference schemes uncovered that using Mixture of Experts techniques brings about an entirely new suite of challenges.\n## Background## Mixture of Experts: A Quick Recap In recent years, there has been a growing interest in methods of increasing the number of parameters in neural networks while maintaining their computational demands. A major advance in eliminating some problems with traditional MoE approaches is Expert Choice (here biblio). Our work has been inspired by their approach - MoT can be seen as an Expert Choice (as opposed to Token Choice) kind of method.\nThere is also concurrent work (Softmoe) investigating ideas similar to ours in the vision domain. It is, however, limited to encoder-only models. It is worth noting that earlier, other fully-differentiable MoE approaches had been proposed (biblio copied from Softmoe). /* jakoś inaczej sformułowac: However, expert-mixing strategies presented in those works incur significant computational overhead. Also, they are token choice instead of expert choice, so different than us. */"
  },
  {
    "objectID": "posts/mixture_of_tokens/plot1sa.html",
    "href": "posts/mixture_of_tokens/plot1sa.html",
    "title": "llm-random",
    "section": "",
    "text": "generic_moe"
  },
  {
    "objectID": "posts/mixture_of_tokens/index.html#implementation",
    "href": "posts/mixture_of_tokens/index.html#implementation",
    "title": "RAW HTML CONTENT",
    "section": "Implementation",
    "text": "Implementation\n\nIn the first phase of our MoT Layer, we partition batch sequences into token gropus of equal size.\nThen for each group, we calculate a two sets of weights, named merge and emit. Each set of weights sum to 1.0, signifying the percentage of each token to utilize.\nFor every group, using its merge weights, we merge belonging tokens into a single token. Following this, we process all merged tokens with linear experts’ weights and a relu activation function. Komentarz: To zdecydowanie trzeba lepiej napisać Employing emit weights, we redistribute resultant token after the experts’ layer back to a group of tokens. Lastly, we join the groups to align with the Feed Forward layer input dimensions."
  },
  {
    "objectID": "posts/mixture_of_tokens/index.html#encoder-vs-decoder",
    "href": "posts/mixture_of_tokens/index.html#encoder-vs-decoder",
    "title": "RAW HTML CONTENT",
    "section": "Encoder vs Decoder",
    "text": "Encoder vs Decoder\nIn encoder-only models, each token may attend to the whole input sequence, whereas decoder-only models only allow attending to the tokens that came before a particular token to simulate the autoregressive decoding used during model inference. This is crucial for the training of decoder-only models, as allowing a token to attend to future tokens would constitute a data leak, preventing successful training. A major problem potentially facing any token-mixing strategy is the possibility of a data leak, making the strategy inappropriate for use with decoder-only models. In MoT, the tokens are mixed across batch, i.e. with other tokens at the same position in other samples in the same training batch. We don’t consider other strategies which would be more suitable in the context of encoders."
  },
  {
    "objectID": "posts/mixture_of_tokens/index.html#experimental-setup",
    "href": "posts/mixture_of_tokens/index.html#experimental-setup",
    "title": "RAW HTML CONTENT",
    "section": "Experimental setup",
    "text": "Experimental setup\nIn our experiments we train the models for \\(250{,}000\\) steps using a cosine LR scheduler with \\(2500\\) steps of warmup, and final LR equal to \\(0.1\\) of the peak LR. The context length is \\(256\\) tokens and the batch size is \\(256\\). We tune the max LR separately for each model."
  },
  {
    "objectID": "posts/mixture_of_tokens/index.html#co-tam-dalej",
    "href": "posts/mixture_of_tokens/index.html#co-tam-dalej",
    "title": "RAW HTML CONTENT",
    "section": "Co tam dalej",
    "text": "Co tam dalej\nContMoE vs dense vs expert choice (GPT-BERT mini) Each run was optimized for that many batches of given size Contmoe is better than dense"
  },
  {
    "objectID": "posts/mixture_of_tokens/index.html#temperature-annealing",
    "href": "posts/mixture_of_tokens/index.html#temperature-annealing",
    "title": "RAW HTML CONTENT",
    "section": "Temperature annealing",
    "text": "Temperature annealing\nIn base MoT setup tokens from various samples in the batch are mingled. This theoretically opens doors for being able to tease out the output of one batch sample by another. However, this may pave the way for potential issues.\nTo tackle this problem, we plan to conduct an experiment wherefinal phase will involve a strategic, gradual reduction of temperature, while simultaneously maintaining a low loss - a process referred to as ‘temerature annealing’. The anticipated outcome is a discrete-routing model which promotes inference parallelization - a move expected to significantly boost the overall efficiency of our processes."
  },
  {
    "objectID": "posts/mixture_of_tokens/index.html#scaling-up-models",
    "href": "posts/mixture_of_tokens/index.html#scaling-up-models",
    "title": "RAW HTML CONTENT",
    "section": "Scaling Up Models",
    "text": "Scaling Up Models\nIn a previous section  we showcased results for a BERT-mini FLOP matched MoT. Our preliminary experiments are also suggesting promising outcomes for larger models. In upcoming weeks we aim to prepare comprehensive comparison of larger models with FLOP matched MoT models."
  },
  {
    "objectID": "posts/mixture_of_tokens/index.html#do-not-forget-about-a-bert",
    "href": "posts/mixture_of_tokens/index.html#do-not-forget-about-a-bert",
    "title": "RAW HTML CONTENT",
    "section": "Do not forget about a BERT",
    "text": "Do not forget about a BERT\nAs our results in this post covers only GPT model, in future work we plan to compare results of MoT on BERT model."
  }
]