%   ****************************************
%

@misc{BengioBPP15,
  author       = {Emmanuel Bengio and
                  Pierre{-}Luc Bacon and
                  Joelle Pineau and
                  Doina Precup},
  title        = {Conditional Computation in Neural Networks for faster models},
  journal      = {CoRR},
  volume       = {abs/1511.06297},
  year         = {2015},
  url          = {http://arxiv.org/abs/1511.06297},
  eprinttype    = {arXiv},
  eprint       = {1511.06297},
  timestamp    = {Sat, 23 Jan 2021 01:19:44 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/BengioBPP15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

%base

@misc{base,
  author       = {Mike Lewis and
                  Shruti Bhosale and
                  Tim Dettmers and
                  Naman Goyal and
                  Luke Zettlemoyer},
  title        = {{BASE} Layers: Simplifying Training of Large, Sparse Models},
  journal      = {CoRR},
  volume       = {abs/2103.16716},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.16716},
  eprinttype    = {arXiv},
  eprint       = {2103.16716},
  timestamp    = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-16716.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{bpr,
  author       = {Carlos Riquelme and
                  Joan Puigcerver and
                  Basil Mustafa and
                  Maxim Neumann and
                  Rodolphe Jenatton and
                  Andr{\'{e}} Susano Pinto and
                  Daniel Keysers and
                  Neil Houlsby},
  title        = {Scaling Vision with Sparse Mixture of Experts},
  journal      = {CoRR},
  volume       = {abs/2106.05974},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.05974},
  eprinttype    = {arXiv},
  eprint       = {2106.05974},
  timestamp    = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-05974.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



%hash

@misc{04426,
  author       = {Stephen Roller and
                  Sainbayar Sukhbaatar and
                  Arthur Szlam and
                  Jason Weston},
  title        = {Hash Layers For Large Sparse Models},
  journal      = {CoRR},
  volume       = {abs/2106.04426},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.04426},
  eprinttype    = {arXiv},
  eprint       = {2106.04426},
  timestamp    = {Fri, 11 Jun 2021 11:04:16 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-04426.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


%stable moe:

@misc{dai2022stablemoe,
      title={StableMoE: Stable Routing Strategy for Mixture of Experts}, 
      author={Damai Dai and Li Dong and Shuming Ma and Bo Zheng and Zhifang Sui and Baobao Chang and Furu Wei},
      year={2022},
      eprint={2204.08396},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


%compare the scores on a lower dimensional manifold

@misc{chi2022representation,
      title={On the Representation Collapse of Sparse Mixture of Experts}, 
      author={Zewen Chi and Li Dong and Shaohan Huang and Damai Dai and Shuming Ma and Barun Patra and Saksham Singhal and Payal Bajaj and Xia Song and Xian-Ling Mao and Heyan Huang and Furu Wei},
      year={2022},
      eprint={2204.09179},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@ARTICLE{smit54,
	AUTHOR = {J. G. Smith and H. K. Weston},
	TITLE = {Nothing Particular in this Year's History},
	YEAR = {1954},
	JOURNAL = {J. Geophys. Res.},
	VOLUME = {2},
	PAGES = {14-15}
}
@misc{muqeeth2023soft,
      title={Soft Merging of Experts with Adaptive Routing}, 
      author={Mohammed Muqeeth and Haokun Liu and Colin Raffel},
      year={2023},
      eprint={2306.03745},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@BOOK{colu92,
	AUTHOR = {Christopher Columbus},
	TITLE = {How {I} Discovered {America}},
	YEAR = {1492},
	PUBLISHER = {Hispanic Press},
	ADDRESS = {Barcelona}
}
@ARTICLE{transformer,
      title={Attention Is All You Need},
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models},
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{dosovitskiy2021image,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{NIPS2015_ae0eb3ee,
 author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning both Weights and Connections for Efficient Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf},
 volume = {28},
 year = {2015}
}
@inproceedings{NIPS1989_6c9882bb,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Damage},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
 volume = {2},
 year = {1989}
}
@misc{frankle2019lottery,
      title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
      author={Jonathan Frankle and Michael Carbin},
      year={2019},
      eprint={1803.03635},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{fedus2022switch,
      title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}, 
      author={William Fedus and Barret Zoph and Noam Shazeer},
      year={2022},
      eprint={2101.03961},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{shazeer2017outrageously,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{puigcerver2023sparse,
      title={From Sparse to Soft Mixtures of Experts}, 
      author={Joan Puigcerver and Carlos Riquelme and Basil Mustafa and Neil Houlsby},
      year={2023},
      eprint={2308.00951},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{zhou2022mixtureofexperts,
      title={Mixture-of-Experts with Expert Choice Routing}, 
      author={Yanqi Zhou and Tao Lei and Hanxiao Liu and Nan Du and Yanping Huang and Vincent Zhao and Andrew Dai and Zhifeng Chen and Quoc Le and James Laudon},
      year={2022},
      eprint={2202.09368},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{du2022glam,
      title={GLaM: Efficient Scaling of Language Models with Mixture-of-Experts}, 
      author={Nan Du and Yanping Huang and Andrew M. Dai and Simon Tong and Dmitry Lepikhin and Yuanzhong Xu and Maxim Krikun and Yanqi Zhou and Adams Wei Yu and Orhan Firat and Barret Zoph and Liam Fedus and Maarten Bosma and Zongwei Zhou and Tao Wang and Yu Emma Wang and Kellie Webster and Marie Pellat and Kevin Robinson and Kathleen Meier-Hellstern and Toju Duke and Lucas Dixon and Kun Zhang and Quoc V Le and Yonghui Wu and Zhifeng Chen and Claire Cui},
      year={2022},
      eprint={2112.06905},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{clark2022unified,
      title={Unified Scaling Laws for Routed Language Models}, 
      author={Aidan Clark and Diego de las Casas and Aurelia Guy and Arthur Mensch and Michela Paganini and Jordan Hoffmann and Bogdan Damoc and Blake Hechtman and Trevor Cai and Sebastian Borgeaud and George van den Driessche and Eliza Rutherford and Tom Hennigan and Matthew Johnson and Katie Millican and Albin Cassirer and Chris Jones and Elena Buchatskaya and David Budden and Laurent Sifre and Simon Osindero and Oriol Vinyals and Jack Rae and Erich Elsen and Koray Kavukcuoglu and Karen Simonyan},
      year={2022},
      eprint={2202.01169},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@InProceedings{pmlr-v119-frankle20a,
  title = 	 {Linear Mode Connectivity and the Lottery Ticket Hypothesis},
  author =       {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3259--3269},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/frankle20a/frankle20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/frankle20a.html},
  abstract = 	 {We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).}
}
@misc{li2017pruning,
      title={Pruning Filters for Efficient ConvNets},
      author={Hao Li and Asim Kadav and Igor Durdanovic and Hanan Samet and Hans Peter Graf},
      year={2017},
      eprint={1608.08710},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{2019t5,
    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    journal = {arXiv e-prints},
    year = {2019},
    archivePrefix = {arXiv},
    eprint = {1910.10683},
}

@misc{he2018soft,
      title={Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks},
      author={Yang He and Guoliang Kang and Xuanyi Dong and Yanwei Fu and Yi Yang},
      year={2018},
      eprint={1808.06866},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{maene2021understanding,
      title={Towards Understanding Iterative Magnitude Pruning: Why Lottery Tickets Win},
      author={Jaron Maene and Mingxiao Li and Marie-Francine Moens},
      year={2021},
      eprint={2106.06955},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{blalock2020state,
      title={What is the State of Neural Network Pruning?},
      author={Davis Blalock and Jose Javier Gonzalez Ortiz and Jonathan Frankle and John Guttag},
      year={2020},
      eprint={2003.03033},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{rae2022scaling,
      title={Scaling Language Models: Methods, Analysis & Insights from Training Gopher},
      author={Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
      year={2022},
      eprint={2112.11446},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{loshchilov2019decoupled,
      title={Decoupled Weight Decay Regularization},
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{nawrot2022hierarchical,
      title={Hierarchical Transformers Are More Efficient Language Models}, 
      author={Piotr Nawrot and Szymon Tworkowski and Michał Tyrolski and Łukasz Kaiser and Yuhuai Wu and Christian Szegedy and Henryk Michalewski},
      year={2022},
      eprint={2110.13711},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{strubell2019energy,
      title={Energy and Policy Considerations for Deep Learning in NLP}, 
      author={Emma Strubell and Ananya Ganesh and Andrew McCallum},
      year={2019},
      eprint={1906.02243},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{jaszczur2021sparse,
      title={Sparse is Enough in Scaling Transformers}, 
      author={Sebastian Jaszczur and Aakanksha Chowdhery and Afroz Mohiuddin and Łukasz Kaiser and Wojciech Gajewski and Henryk Michalewski and Jonni Kanerva},
      year={2021},
      eprint={2111.12763},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{hoffmann2022training,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization},
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{he2015delving,
      title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1502.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
