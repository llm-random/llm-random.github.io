---
title: "Mixture of Tokens"
author:
    -   name: Szymon Antoniak \*
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Michał Krutul
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Maciej Pióro
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Jakub Krajewski
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Jan Ludziejewski
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Tomasz Odrzygóźdź
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Marek Cygan ‡
        affiliations:
        -   ref: uow
        -   ref: nomagic
    -   name: Sebastian Jaszczur †
        affiliations:
        -   ref: ideas
        -   ref: uow
affiliations:
    -   id: ideas
        name: IDEAS NCBR
    -   id: uow
        name: University of Warsaw
    -   id: nomagic
        name: Nomagic
date: "2023-09-29"
bibliography: bibliography.bib
# csl: apa.csl
# date-modified: "2023-09-29"
categories: []
image: "logo_mot.png"
format:
  html:
    template-partials:
    - title-block.html
    include-in-header:
      - text: |
          <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.20.0.min.js"> </script>
          <style> 
          
            .MK {
            color: brown;
            }
            .MK::before {
            content: "[MK] ";
            }

            .SA {
            color: purple;
            }
            .SA::before {
            content: "[SA] ";
            }

            .MP {
            color: teal;
            }
            .MP::before {
            content: "[MP] ";
            }

            .JK {
            color: coral;
            }
            .JK::before {
            content: "[JK] ";
            }

            .JL {
            color: green;
            }
            .JL::before {
            content: "[JL] ";
            }

            .TO {
            color: mediumslateblue;
            }
            .TO::before {
            content: "[TO] ";
            }

            .MC {
            color: red;
            }
            .MC::before {
            content: "[MC] ";
            }

            .SJ {
            color: goldenrod;
            }
            .SJ::before {
            content: "[SJ] ";
            }
          </style>
---

# TEST

[new text]{.SA}~~old text~~

[new text]{.MK}~~old text~~

[new text]{.MP}~~old text~~

[new text]{.JK}~~old text~~

[new text]{.JL}~~old text~~

[new text]{.TO}~~old text~~

[new text]{.SJ}~~old text~~

[new text]{.MC}~~old text~~

# Introduction 
Mixture of Experts [(MoE)]{.MK} architectures have recently garnerned considerable attention for their ability to increase the size of Transformer architectures while keeping the computational cost of training and inference constant. The most successful MoE approaches achieve this by activating only subsets of a very large feed forward layer for each processed token. 
[Komentarz: Czy są MoE które nie wybierają tylko części parametrów FF dla danego tokena? ]{.MK}

This technique comes at a cost, though: the choice of that subset of parameters for a given token is a discrete operation, and as such its training requires either the use of gradient estimators for traditional gradient-based optimisation or non-gradient based techniques; the models are known to suffer from issues such as training instability, under- and overload of _experts_ (subsets of the FeedForward layer) and more. While some of those problems can be alleviated with e.g. the use of various auxiliary losses or reduced initialisation scale, it is certain that existing MoE techniques are more difficult and less intuitive to train. 

With this motivation, we propose Mixture of Tokens: a new, fully-differentiable type of architecture that retains all efficiency benefits of MoE and alleviates the aformentioned problem by mixing _tokens_ before feeding them into the FeedForward layer. This technique is fully compatible with both masked and causual LLM training.

# Motivation 
## Scaling with Mixture of Experts 
Existing [(MoE)]{.MK}~~Mixture of Experts~~ approaches have demonstrated very impressive results in training huge models for language [@Switch], vision [@moe_vision] and more [@moe_multimodal]. The core idea is to replace [ the standard FeedForward layer from ]{.MK} ~~the FeedForward layer standard for~~ Transformer architectures with a set of _experts_, usually also MLPs, and have each token [to]{.MK} be processed by a subset of experts. 

{{< include plot1sa.qmd >}}

The increased model size, despite keeping computational cost during training and inference constant, leads to significant performance improvements, and has been shown to behave predictably with respect to parameter count and inference compute [scaling moe], similarly to the standard dense Transformer models [scaling,chinchilla]. 

However, following the success of those initial huge parameter-count models, thorough analysis of the model training and inference schemes uncovered that using Mixture of Experts techniques brings about an entirely new suite of challenges.


[## Background]{.JK}~~## Mixture of Experts: A Quick Recap~~
In recent years, there has been a growing interest in methods of increasing the number of parameters in neural networks while maintaining their computational demands. A major advance in eliminating some problems with traditional MoE approaches is Expert Choice (here biblio). Our work has been inspired by their approach - MoT can be seen as an Expert Choice (as opposed to *Token Choice*) kind of method.


There is also concurrent work (Softmoe) investigating ideas similar to ours in the vision domain. It is, however, limited to encoder-only models. It is worth noting that earlier, other fully-differentiable MoE approaches had been proposed (biblio copied from Softmoe). /* jakoś inaczej sformułowac: However, expert-mixing strategies presented in those works incur significant computational overhead. Also, they are token choice instead of expert choice, so different than us. */

## Limitations of Current Approaches (sekcja 8.2 w paperze) SA 
<!-- JK - byłoby bardziej logicze dać tę sekcję pod related work -->
Without mentioning papers 
- obecne podejścia są niestabilne
- mamy idealny balancing token -> expert

with training stability, management of expert _load_ (how many tokens are routed to each expert) that lead to time overheads, mode collapse (all tokens routed to a small portion of the expert set), leaving some tokens unprocessed, problems with expert exploration and specialization/redundancy, low performance of discrete operations on accelerators and finally communication costs in parallel training.



# Method 
- Diagram SJ

{{< include diagrams/general1.qmd >}} 


## Implementation
<!-- MK -->
In the first phase of our MoT Layer, we partition batch sequences into token gropus of equal size. 

Then for each group, we calculate a two sets of weights, named _merge_ and _emit_. Each set of weights sum to 1.0, signifying the percentage of each token to utilize.

For every group, using its _merge_ weights, we merge belonging tokens into a single token. Following this, we process all merged tokens with linear experts' weights and a relu activation function. [Komentarz: To zdecydowanie trzeba lepiej napisać]{.MK} 
Employing _emit_ weights, we redistribute resultant token after the experts' layer back to a group of tokens. Lastly, we join the groups to align with the Feed Forward layer input dimensions.

## Encoder vs Decoder
In encoder-only models, each token may attend to the whole input sequence, whereas decoder-only models only allow attending to the tokens that came before a particular token to simulate the autoregressive decoding used during model inference. This is crucial for the training of decoder-only models, as allowing a token to attend to future tokens would constitute a data leak, preventing successful training.
A major problem potentially facing any token-mixing strategy is the possibility of a data leak, making the strategy inappropriate for use with decoder-only models. In MoT, the tokens are mixed across batch, i.e. with other tokens at the same position in other samples in the same training batch. We don't consider other strategies which would be more suitable in the context of encoders.


# Experiments MP / SA
## Experimental setup
In our experiments we train the models for $250{,}000$ steps using a cosine LR scheduler with $2500$ steps of warmup, and final LR equal to $0.1$ of the peak LR. The context length is $256$ tokens and the batch size is $256$.
We tune the max LR separately for each model.

![Performance by step](loss_step.png)
![Performance by step](html-figure.png)

## Co tam dalej
ContMoE vs dense vs expert choice (GPT-BERT mini)
Each run was optimized for that many batches of given size
Contmoe is better than dense


# Upcoming Work
<!-- MK Kto wie czy to dobra nazwa, może Future Work lepsza -->
Coming up in the following weeks, we're ready to take our work up a notch with a few advancements that we're covering in this segment.

## Temperature annealing
In base MoT setup tokens from various samples in the batch are mingled. This theoretically opens doors for being able to tease out the output of one batch sample by another. However, this may pave the way for potential issues.

To tackle this problem, we plan to conduct an experiment wherefinal phase will involve a strategic, gradual reduction of temperature, while simultaneously maintaining a low loss - a process referred to as 'temerature annealing'. The anticipated outcome is a discrete-routing model which promotes inference parallelization - a move expected to significantly boost the overall efficiency of our processes.

## Scaling Up Models
In a previous section <TODO check> we showcased results for a BERT-mini FLOP matched MoT. Our preliminary experiments are also suggesting promising outcomes for larger models. In upcoming weeks we aim to prepare comprehensive comparison of larger models with FLOP matched MoT models.


## Do not forget about a BERT
As our results in this post covers only GPT model, in future work we plan to compare results of MoT on BERT model. 


# Conclusions SJ
We have shown the preliminary results showing the promise of MoT improving both the stability of training and final performance of the model. More thorough experiments are underway at the moment and we will release the paper with more results in the coming weeks. In the meantime, you can contact us with any feedback you have at llm.random.team@gmail.com .