---
title: "Mixture of Tokens"
author:
    -   name: Szymon Antoniak&nbsp;
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Sebastian Jaszczur&nbsp;†
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Michał Krutul
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Maciej Pióro
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Jakub Krajewski
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Jan Ludziejewski
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Tomasz Odrzygóźdź
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Marek Cygan&nbsp;‡
        affiliations:
        -   ref: uow
        -   ref: nomagic
affiliations:
    -   id: ideas
        name: IDEAS NCBR
    -   id: uow
        name: University of Warsaw
    -   id: nomagic
        name: Nomagic
date: "2023-10-06"
bibliography: bibliography.bib
# csl: apa.csl
# date-modified: "2023-09-29"
categories: []
image: "logo_mot.png"
format:
  html:
    template-partials:
    - title-block.html
    include-in-header:
      - text: |
          <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.20.0.min.js"> </script>
          <style> 
          
            .MK {
            display: none;
            color: brown;
            }
            .MK::before {
            display: none;
            content: "[MK] ";
            }

            .SA {
            display: none;
            color: goldenrod;
            }
            .SA::before {
            display: none;
            content: "[SA] ";
            }

            .MP {
            display: none;
            color: teal;
            }
            .MP::before {
            display: none;
            content: "[MP] ";
            }

            .JK {
            display: none;
            color: coral;
            }
            .JK::before {
            display: none;
            content: "[JK] ";
            }

            .JL {
            display: none;
            color: green;
            }
            .JL::before {
            display: none;
            content: "[JL] ";
            }

            .TO {
            display: none;
            color: purple;
            }
            .TO::before {
            display: none;
            content: "[TO] ";
            }

            .MC {
            display: none;
            color: red;
            }
            .MC::before {
            display: none;
            content: "[MC] ";
            }

            .SJ {
            display: none;
            color: blue;
            }
            .SJ::before {
            display: none;
            content: "[SJ] ";
            }
          </style>
---

<!-- # TEST

[new text]{.SA}~~old text~~

[new text]{.MK}~~old text~~

[new text]{.MP}~~old text~~

[new text]{.JK}~~old text~~

[new text]{.JL}~~old text~~

[new text]{.TO}~~old text~~

[new text]{.SJ}~~old text~~

[new text]{.MC}~~old text~~

# Standardization 
-  feed-forward 
- American  -->



# Introduction 
Mixture of Experts (MoE) architectures have recently garnerned considerable attention for their ability to increase the size of Transformer models while keeping the computational cost of training and inference constant. The most successful MoE approaches achieve this by activating only subsets of a very large feed forward layer for each processed token (the alternative subsets of parameters are often called _experts_). 

This technique comes at a cost, though: the operation of choosing the most suitable experts for a given token is discrete, and learning discrete choices is difficult; the models are known to suffer from issues including training instability and expert under- and overload. While some of those problems can be alleviated with e.g. the use of various auxiliary losses or reduced initialisation scale, it is certain that existing MoE techniques are more difficult and less intuitive to train than dense counterparts.

Aiming to avoid these problems we propose **Mixture of Tokens**: a new, fully-differentiable type of architecture that retains all efficiency benefits of MoE and alleviates the aformentioned problems by _mixing tokens_ before feeding them into the feed-forward layer instead of routing tokens to experts, in effect allowing the model to simultaneously learn from all token-expert combinations. Crucially, this technique is fully compatible with both masked and causal LLM training. 

[GENERAL TODOS: grammarly, go over and rephrase, align and make clearer the Methods section, make mail, tweet, settle on contribution attributions, ]{.SA}

[zastanowić się nad kolejnością autorów, głównie SJ]{.SJ}

[jakiś fajniejszy opis tekstu nas tronce głównej dać. Można przez display:none'd div.]{.SJ}

# Motivation 
## Scaling Language Models
Numerous [TODO SA poprawia]{.SA} works [@kaplan2020scaling; @hoffmann2022training] have studied benefits of increasing the size of language models for their performance. However, this scaling comes at a very high computational cost [@strubell2019energy]. Therefore, efficient use of computational resources and network parameters is often recognized as one of the most important challenges in the area [TODO moze lepsze słowo tutaj SA]{.SA} [@rae2022scaling; @jaszczur2021sparse; @nawrot2022hierarchical]. One of the most promising research directions in this domain is conditional computation, meaning methods that involve activating only a subset of parameters for a given input. In the context of language models, this idea has been most successfully implemented in MoE, leading to huge increases in model performance.
 
## Mixture of Experts
The core idea in MoE is to increase model size without incurring additional computational costs. Roughly speaking, it is done by replacing the feed-forward layer standard for Transformer architectures with a (potentially very large) set of experts. In this new setup, a given token is processed only by a small subset of experts, and that selection is made by a small network called a *controller* (sometimes called a *router*). 
The change in model quality due to this increase in model size has been shown to behave predictably with respect to parameter count and inference compute [@clark2022unified]. More detailed background and explanation of variants of the MoE architecture is presented in [Background](#sec-back).


## Limitations of current approaches 
Despite promising results, wider adoption of MoE has been limited by a number of restrictions of the architecture and difficulties in the training. Among these, the most important are: 
[przeppisać ten paragraf]{.SA}

* **Training instability.** Numerous works [@fedus2022switch; @du2022glam] have reported difficulties in training MoE models due to instabilities. This is most likely connected with the discrete nature of the technique: the operation of choosing top-k most relevant tokens/experts in non-differentiable.We hypothesise that existing techniques used for training the cotnroller with gradient descent result in lower training stability.
* **Load imbalance.** Typically, in MoE we set the maximum *capacity* for each expert. However, we are not able to efficiently restrict the choice of the routing network to assign tokens in a perfectly balanced way. This leads to *token dropping* (when some tokens are not processed at all) and *expert under-training* (when some experts receive less data and are therefore not trained enough). [mention underspecialization as well: ]{.SJ} [there is work by Basil that does some balancing for that. Will liink later]{.SA}
* **Information leak.** Some of the most successful MoE methods process tokens from different positions in a sequence together (i.e. by comparing scores of all tokens in a batch). This imposes an information leak, and hinders their utility in autoregressive decoding. [remember that we leak in another way, so don't be too harsh here, covering a large part of use cases in modern language models.]{.SA} [SJ edit, mention just industry applications]{.SJ}<!--[SIMON ANOTHER LINK!!!!!!]-->


<!-- We want to address these problems using our techique, Mixture of Tokens. -->
<!-- JK - tu można dorzucić nierówne traktowanie przykładów (niektóre przykłądy dostają więcej compute). Można też dać, ze niektóre techniki nie proponują rozwiązania, które byłoby użyteczne dla generative decoding, ale trzeba by dać related work powyżej -->

<!-- JK - byłoby bardziej logicze dać tę sekcję pod related work -->
<!--Without mentioning papers 
- obecne podejścia są niestabilne
- mamy idealny balancing token -> expert

with training stability, management of expert _load_ (how many tokens are routed to each expert) that lead to time overheads, mode collapse (all tokens routed to a small portion of the expert set), leaving some tokens unprocessed, problems with expert exploration and specialization/redundancy, low performance of discrete operations on accelerators and finally communication costs in parallel training.-->


# Background {#sec-back}
In the context of language models, Mixture of Experts has been originally proposed in [@shazeer2017outrageously]. Basic overview of the technique is presented in the diagram below: instead of processing each token with the standard feed-forward layer, we choose one of multiple possible experts. 

[This section should be vastly improved]{.SJ}

<!-- {{< include diagrams/_generic_moe.qmd >}} -->

The technique was further simplified by [@fedus2022switch], proposing to send each token to only *one* expert with the highest score produced by the routing network. In both cases there was a need to use auxiliary losses to encourage exploration and mitigate load imbalance across experts. More recently, [@zhou2022mixtureofexperts] proposed a different approach of choosing *Top-k tokens for each expert* instead. This means that different tokens might be attended to by varying number of experts. The difference between both approaches, *token choice* and *expert choice*, is illustrated in the diagram below.

{{< include diagrams/_vanillamoes.qmd >}}
[TODO SJ: dodac wykres vanilla/OG transformer]{.SJ}

[TODO SJ poprawia]{.SJ}

Concurrently to our work, [@puigcerver2023sparse] proposed a continuous variant of Mixture of Experts for the Vision Transformer, as such limited to encoder-only models. Another approach allowing to avoid discrete operations in MoE by merging experts was presented in [@muqeeth2023soft].



<!-- JK- czy były inne soft wersje moe? -->

<!-- [This needs to be quite comprehensive, what we see below unnecessarily focuses on Expert Choice and Softmoe]{.SA}
In recent years, there has been a growing interest in methods of increasing the number of parameters in neural networks while maintaining their computational demands. A major advance in eliminating some problems with traditional MoE approaches is Expert Choice (here biblio). Our work has been inspired by their approach - Mixture of Tokens can be seen as an Expert Choice (as opposed to *Token Choice*) kind of method.

There is also concurrent work (Softmoe) nvestigating ideas similar to ours in the vision domain. It is, however, limited to encoder-only models. It is worth noting that earlier, other fully-differentiable MoE approaches had been proposed (biblio copied from Softmoe). /* jakoś inaczej sformułowac: However, expert-mixing strategies presented in those works incur significant computational overhead. Also, they are token choice instead of expert choice, so different than us. */ -->


# Method
<!-- In this section we provide an overview of our proposed method. More detailed description of all operations can be found in the next section. [T U T A J L I N K]{.SA} -->

[Link to diagrams: https://drive.google.com/file/d/1jvew0ft43zpaG-iCk7Ob2ilJw7VjsyeZ/view?usp=sharing ]{.SJ}
<!-- 
[Move this paragraph to background]{.SJ}
The general idea of Mixture of Tokens is to run multiple experts which independently aggregate and process tokens. The most common (and original) form of MoE, token-choice, operates from the perspective of a single token. [This paragraph will be more detailed in background section.]{.SJ} However, Mixture of Tokens is more similar to expert-choice, where it is an expert who decides which tokens to process.

[comparison of both token-choice and expert-choice]{.SJ}

{{< include diagrams/_vanillamoes.qmd >}}  -->

In Mixture of Tokens, all experts work independently from one another and act similar to a normal feed-forward layer. The difference in our setup, compared to MoE, is that a single expert receives a weighted average of multiple tokens. Then, output from an expert is rescaled and redistributed to the original tokens. See the diagrams below. [SA stylistycznie]{.SA}
{{< include diagrams/_general1.qmd >}} 

{{< include diagrams/_ffn1.qmd >}}


Weights for merging and emitting groups of tokens are calculated using _controller_ and _softmax_. 

{{< include diagrams/_mergeemit.qmd >}}

There are multiple possible ways to group tokens. However, to be able to use Mixture of Tokens in autoregressive decoder we group tokens by position as presented in the diagram below.

{{< include diagrams/_grouping.qmd >}}


<!-- In Mixture of Tokens, each expert works independently from one another. [The only dependence is during experts' outputs aggregation, but it's a matter of rescaling each output before adding it to the residual, so we will skip it/explain it in later section.]{.SJ} Therefore, to explain the inner workings of Mixture of Tokens we will focus on a single expert. See the diagram below.


{{< include diagrams/_general1.qmd >}} 


This single expert works as a normal feedforward layer[(maybe a diagram of lin1->relu->lin2?)]{.SJ}, except for a merger and an emitter placed to process the input and the output, respectively. Each merger takes a group of tokens and computes a weighted average of them ~~(according to weights described in later section)~~. 



~~Now we get to the part of computing weights.~~
 The weights are the same [(are they really?)]{.SJ}[It depends on a version]{.MK} for both merger and emitter.



How can we apply this technique in decoder? We need a kind of grouping that will not leak information into future tokens. We can achieve that by grouping tokens by position.



{{< include diagrams/_grouping.qmd >}} -->


<!-- ## Implementation
In the first phase of our Mixture of Tokens Layer, we partition batch sequences into token gropus of equal size. 
Then for each group, we calculate a two sets of weights, named _merge_ and _emit_. Each set of weights sum to 1.0, signifying the percentage of each token to utilize.

For every group, using its _merge_ weights, we merge belonging tokens into a single token. Following this, we process all merged tokens with linear experts' weights and a relu activation function. [Komentarz: To zdecydowanie trzeba lepiej napisać]{.MK} 
Employing _emit_ weights, we redistribute resultant token after the experts' layer back to a group of tokens. Lastly, we join the groups to align with the Feed Forward layer input dimensions.

## Encoder vs Decoder
In encoder-only models,   ach token may attend to the whole input sequence, whereas decoder-only models only allow attending to the tokens that came before a particular token to simulate the autoregressive decoding used during model inference. This is crucial for the training of decoder-only models, as allowing a token to attend to future tokens would constitute a data leak, preventing successful training.
A major problem potentially facing any token-mixing strategy is the possibility of a data leak, making the strategy inappropriate for use with decoder-only models. In Mixture of Tokens, the tokens are mixed across batch, i.e. with other tokens at the same position in other samples in the same training batch. We don't consider other strategies which would be more suitable in the context of encoders. -->
 

# Experiments
## Experimental setup
For the baseline, we train a standard GPT-like model on the language modeling task using cross entropy loss on the C4 dataset [@2019t5]. Our model replaces all feed-forward layers with MoT layers.

For proof-of-concept experiments, we chose model size identical to **BERT-mini**:

::: {.callout-note icon=false}
## Model Hyperparameters

* 4 layers
* 256 embedding dimension
* 1024 hidden dimension
* 4 attention heads
:::

::: {.callout-note icon=false}
## Training Setup

* 250K training steps
* 256 context length
* 256 batch size
* lr warmup for the first 1% of training
* cosine scheduling to 0.1 of peak lr at the end of training
:::

::: {.callout-note icon=false}
## Mixture of Tokens hyperparameters

* 32 group size
* 512 experts
* 32x more parameters than a dense feed-forward layer
:::

The learning rate was tuned separately for both our model and the baseline:

::: {.callout-note icon=false}
## Learning Rate

* Dense baseline: 4e-3
* Mixture of Tokens: 2e-3
:::




## Results

{{< include diagrams/_step.qmd >}}
Our Mixture of Tokens model attains the dense models' final training loss in just 24% of steps.
[update plot SA after training finished]{.SA}
[ZTODO zmienić 24% of traning steps na czarny kolor i tę kreseczkę też, przerywana linia też w innym kolorze]{.SA}



{{< include diagrams/_time.qmd >}}
If we measure actual training time, the final dense model loss is achieved in only 33% of time.
[update plot SA after training finished]{.SA}
[ZTODO zmienić 33% of traning steps na czarny kolor i tę kreseczkę też, przerywana linia też w innym kolorze]{.SA}



# Next steps

## Scaling Up  
Our preliminary experiments suggest that Mixture of Tokens might work even better for larger model sizes. In upcoming weeks we aim to prepare comprehensive comparison of larger models and compare them to Mixture of Experts methods.

## From Mixture of Tokens to Mixture of Experts


In the default Mixture of Tokens setup for autoregressive training, tokens from different examples in the batch are mixed. This, in principle, opens a possibility of teasing out the output of one sample from another in the same batch - which might be undesirable in some use cases.

To quantify the rate of token mixing we investigated the evolution of the mean entropy of weights produced by Mixture of Tokens layers. In other words: in each layer, we average the entropy of weights over all token groups and all experts. 

For the base version of MoT, the results are as follows:

{{< include diagrams/_temp_baseline.qmd >}}

::: {.callout-tip}
## Backward Compatibility

The controller produces merging weights by taking a softmax over scores calculated independently for each token. If it should happen that one score would dominate, for example by having very low temperature, the mixing operation would focus on exclusively one token. This, in effect, would make Mixture of Tokens equivalent to Token-Choice architecture.

:::

As it turns out, allowing the temperature to be learnable, we effectively allow the model to lower rate of token mixing, especially in later layers:

{{< include diagrams/_temp_learnable.qmd >}}

Interestingly, this comes at a cost of model performance.

{{< include diagrams/_step_temp_loss.qmd >}}

Thus, allowing the temperature to be learned near the end of training seems to be a very promising direction for "private" autoregressive decoding: this way, we retain all the benefits of training with a high rate of token mixing, and have very low rates during inference.

[# TODO: dodać uczalną temperaturę na koniec treningu (te eksperymenty już mamy)]{.SA}



[obrazki z różnymi temperaturami softmax ]{.MK}


# Conclusions SJ
We have shown the preliminary results showing the promise of Mixture of Tokens improving both the stability of training and final performance of the model. More thorough experiments are underway at the moment and we plan to release the paper with more results in the coming weeks. In the meantime, you can contact us with any feedback you have at llm.random.team@gmail.com .[add twitter begging rwetweet us.]{.SJ}