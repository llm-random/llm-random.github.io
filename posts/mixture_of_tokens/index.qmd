---
title: "Mixture of Tokens"
author:
    -   name: Szymon Antoniak&nbsp;
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Sebastian Jaszczur&nbsp;†
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Michał Krutul
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Maciej Pióro
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Jakub Krajewski
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Jan Ludziejewski
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Tomasz Odrzygóźdź
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Marek Cygan&nbsp;‡
        affiliations:
        -   ref: uow
        -   ref: nomagic
affiliations:
    -   id: ideas
        name: IDEAS NCBR
    -   id: uow
        name: University of Warsaw
    -   id: nomagic
        name: Nomagic
date: "2023-10-06"
bibliography: bibliography.bib
# csl: apa.csl
# date-modified: "2023-09-29"
categories: []
image: "logo_mot.png"
format:
  html:
    template-partials:
    - title-block.html
    include-in-header:
      - text: |
          <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.20.0.min.js"> </script>
          <style> 
          
            .MK {
            display: none;
            color: brown;
            }
            .MK::before {
            display: none;
            content: "[MK] ";
            }

            .SA {
            display: none;
            color: goldenrod;
            }
            .SA::before {
            display: none;
            content: "[SA] ";
            }

            .MP {
            display: none;
            color: teal;
            }
            .MP::before {
            display: none;
            content: "[MP] ";
            }

            .JK {
            display: none;
            color: coral;
            }
            .JK::before {
            display: none;
            content: "[JK] ";
            }

            .JL {
            display: none;
            color: green;
            }
            .JL::before {
            display: none;
            content: "[JL] ";
            }

            .TO {
            display: none;
            color: purple;
            }
            .TO::before {
            display: none;
            content: "[TO] ";
            }

            .MC {
            display: none;
            color: red;
            }
            .MC::before {
            display: none;
            content: "[MC] ";
            }

            .SJ {
            display: none;
            color: blue;
            }
            .SJ::before {
            display: none;
            content: "[SJ] ";
            }
          </style>
---

<!-- # TEST

[new text]{.SA}~~old text~~

[new text]{.MK}~~old text~~

[new text]{.MP}~~old text~~

[new text]{.JK}~~old text~~

[new text]{.JL}~~old text~~

[new text]{.TO}~~old text~~

[new text]{.SJ}~~old text~~

[new text]{.MC}~~old text~~

# Standardization 
-  feed-forward 
- American  -->

[GENERAL TODOS: grammarly, go over and rephrase, align and make clearer the Methods section, make mail, tweet, settle on contribution attributions,add Batch Priority Routing ]{.SA}

[zastanowić się nad kolejnością autorów, głównie SJ]{.SJ}

# Introduction 
Mixture of Experts (MoE) architectures have recently garnerned considerable attention for their ability to increase the size of Transformer models while keeping the computational cost of training and inference constant. The most successful MoE approaches achieve this by activating only subsets of a very large feed forward layer for each processed token (the alternative subsets of parameters are often called _experts_). 

This technique comes at a cost, though: the operation of choosing the most suitable experts for a given token is discrete, and learning discrete choices is difficult; the models are known to suffer from issues including training instability and expert under- and overload. While some of those problems can be alleviated with e.g. the use of various auxiliary losses or reduced initialisation scale, it is certain that existing MoE techniques are more difficult and less intuitive to train than dense counterparts.

Aiming to avoid these problems we propose **Mixture of Tokens**: a new, fully-differentiable type of architecture that retains all efficiency benefits of MoE and alleviates the aformentioned problems by _mixing tokens_ before feeding them into the feed-forward layer instead of routing tokens to experts, in effect allowing the model to simultaneously learn from all token-expert combinations. Crucially, this technique is fully compatible with both masked and causal LLM training. 

# Motivation 
## Scaling Language Models
Large language models based on Transformers currently make up one of the most active fields in ​​Artificial Intelligence, exhibiting human-level performance in a variety of tasks. This is in large part due to their scaling properties - [@kaplan2020scaling; @hoffmann2022training] showed that an increase in model size results in a predictable increase in performance. This scaling leads to an ever-growing demand for computational resources, with their effective utilization often deemed as one of the critical challenges of the field [@rae2022scaling; @jaszczur2021sparse; @nawrot2022hierarchical].


## Mixture of Experts
The core idea in MoE is to increase model size without incurring additional computational costs. Roughly speaking, it is done by replacing the feed-forward layer standard for Transformer architectures with a (potentially very large) set of experts. In this new setup, a given token is processed only by a small subset of experts, and that selection is made by a small network called a *controller*. [^1] 
Increasing model size using MoE architectures has also been shown to scale with parameter count [@clark2022unified]. More detailed background and explanation of variants of the MoE architecture is presented in [Background](#sec-back).

[^1]: *router* is also commonly used in MoE literature.


## Limitations of current approaches 
While the performance of the huge-parameter-count MoE architectures is impressive, they come with an entirely new set of challenges, during both training and inference. The most notable include:

* **Training instability.** Multiple studies [@fedus2022switch; @du2022glam; @limoe] have reported difficulties in training MoE models due to instabilities. This is likely due to with the nature of the technique: the operation of choosing top-k most relevant tokens/experts in discrete, and thus small changes of controller weights can have disproportional effects on controller decisions. We hypothesise that existing techniques used for training the cotnroller with gradient descent, while somewhat effective, do not entirely solve this problem.

* **Load imbalance.** Typically, in MoE we set the maximum *capacity* for each expert. However, we are not able to efficiently restrict the choice of the routing network to assign tokens in a perfectly balanced way. This leads to *token dropping* (when some tokens are not processed by an expert) and *mode collapse* (when the cotroller sends almost all tokens to a few experts) [@The Sparsely-Gated Mixture-of-Experts Layer].

* **Information leak.** Some of the most successful MoE methods process tokens from different positions in a sequence together (i.e. by comparing scores of all tokens in a batch). This imposes an information leak, and hinders their utility in autoregressive decoding. 


# Background {#sec-back}
In the context of language models, Mixture of Experts has been originally proposed in [@shazeer2017outrageously]. Basic overview of the technique is presented in the diagram below: instead of processing each token with the standard feed-forward layer, we choose one of multiple possible experts. 

[This section should be vastly improved]{.SJ}

<!-- {{< include diagrams/_generic_moe.qmd >}} -->

The technique was further simplified by [@fedus2022switch], proposing to send each token to only *one* expert with the highest score produced by the routing network. In both cases there was a need to use auxiliary losses to encourage exploration and mitigate load imbalance across experts. More recently, [@zhou2022mixtureofexperts] proposed a different approach of choosing *Top-k tokens for each expert* instead. This means that different tokens might be attended to by varying number of experts. The difference between both approaches, *token choice* and *expert choice*, is illustrated in the diagram below.

{{< include diagrams/_vanillamoes.qmd >}}
[TODO SJ: dodac wykres vanilla/OG transformer]{.SJ}

[TODO SJ poprawia]{.SJ}

Concurrently to our work, [@puigcerver2023sparse] proposed a continuous variant of Mixture of Experts for the Vision Transformer, as such limited to encoder-only models. Another approach allowing to avoid discrete operations in MoE by merging experts was presented in [@muqeeth2023soft].



# Method

[Link to diagrams: https://drive.google.com/file/d/1jvew0ft43zpaG-iCk7Ob2ilJw7VjsyeZ/view?usp=sharing ]{.SJ}

In Mixture of Tokens, all experts work independently from one another and act similar to a normal feed-forward layer. The difference in our setup, compared to MoE, is that a single expert receives a weighted average of multiple tokens. Then, output from an expert is rescaled and redistributed to the original tokens. See the diagrams below. [SA stylistycznie]{.SA}
{{< include diagrams/_general1.qmd >}} 

{{< include diagrams/_ffn1.qmd >}}


Weights for merging and emitting groups of tokens are calculated using _controller_ and _softmax_. 

{{< include diagrams/_mergeemit.qmd >}}

There are multiple possible ways to group tokens. However, to be able to use Mixture of Tokens in autoregressive decoder we group tokens by position as presented in the diagram below.

{{< include diagrams/_grouping.qmd >}}


<!-- In Mixture of Tokens, each expert works independently from one another. [The only dependence is during experts' outputs aggregation, but it's a matter of rescaling each output before adding it to the residual, so we will skip it/explain it in later section.]{.SJ} Therefore, to explain the inner workings of Mixture of Tokens we will focus on a single expert. See the diagram below.


{{< include diagrams/_general1.qmd >}} 


This single expert works as a normal feedforward layer[(maybe a diagram of lin1->relu->lin2?)]{.SJ}, except for a merger and an emitter placed to process the input and the output, respectively. Each merger takes a group of tokens and computes a weighted average of them ~~(according to weights described in later section)~~. 



~~Now we get to the part of computing weights.~~
 The weights are the same [(are they really?)]{.SJ}[It depends on a version]{.MK} for both merger and emitter.



How can we apply this technique in decoder? We need a kind of grouping that will not leak information into future tokens. We can achieve that by grouping tokens by position.



{{< include diagrams/_grouping.qmd >}} -->


<!-- ## Implementation
In the first phase of our Mixture of Tokens Layer, we partition batch sequences into token gropus of equal size. 
Then for each group, we calculate a two sets of weights, named _merge_ and _emit_. Each set of weights sum to 1.0, signifying the percentage of each token to utilize.

For every group, using its _merge_ weights, we merge belonging tokens into a single token. Following this, we process all merged tokens with linear experts' weights and a relu activation function. [Komentarz: To zdecydowanie trzeba lepiej napisać]{.MK} 
Employing _emit_ weights, we redistribute resultant token after the experts' layer back to a group of tokens. Lastly, we join the groups to align with the Feed Forward layer input dimensions.

## Encoder vs Decoder
In encoder-only models,   ach token may attend to the whole input sequence, whereas decoder-only models only allow attending to the tokens that came before a particular token to simulate the autoregressive decoding used during model inference. This is crucial for the training of decoder-only models, as allowing a token to attend to future tokens would constitute a data leak, preventing successful training.
A major problem potentially facing any token-mixing strategy is the possibility of a data leak, making the strategy inappropriate for use with decoder-only models. In Mixture of Tokens, the tokens are mixed across batch, i.e. with other tokens at the same position in other samples in the same training batch. We don't consider other strategies which would be more suitable in the context of encoders. -->
 

# Experiments
## Experimental setup
For the baseline, we train a standard GPT-like model on the language modeling task using cross entropy loss on the C4 dataset [@2019t5]. Our model replaces all feed-forward layers with MoT layers.

For proof-of-concept experiments, we chose model size identical to **BERT-mini**:

::: {.callout-note icon=false}
## Model Hyperparameters

* 4 layers
* 256 embedding dimension
* 1024 hidden dimension
* 4 attention heads
:::

::: {.callout-note icon=false}
## Training Setup

* 250K training steps
* 256 context length
* 256 batch size
* lr warmup for the first 1% of training
* cosine scheduling to 0.1 of peak lr at the end of training
:::

::: {.callout-note icon=false}
## Mixture of Tokens hyperparameters

* 32 group size
* 512 experts
* 32x more parameters than a dense feed-forward layer
:::

The learning rate was tuned separately for both our model and the baseline:

::: {.callout-note icon=false}
## Learning Rate

* Dense baseline: 4e-3
* Mixture of Tokens: 2e-3
:::




## Results

{{< include diagrams/_step.qmd >}}
Our Mixture of Tokens model attains the dense models' final training loss in just 24% of steps.
[ZTODO zmienić 24% of traning steps na czarny kolor i tę kreseczkę też, przerywana linia też w innym kolorze]{.SA}



{{< include diagrams/_time.qmd >}}
If we measure actual training time, the final dense model loss is achieved in only 33% of time.
[ZTODO zmienić 33% of traning steps na czarny kolor i tę kreseczkę też, przerywana linia też w innym kolorze]{.SA}



# Next steps

## Scaling Up  
Our preliminary experiments suggest that Mixture of Tokens might work even better for larger model sizes. In upcoming weeks we aim to prepare comprehensive comparison of larger models and compare them to Mixture of Experts methods.

## From Mixture of Tokens to Mixture of Experts


In the default Mixture of Tokens setup for autoregressive training, tokens from different examples in the batch are mixed. This, in principle, opens a possibility of teasing out the output of one sample from another in the same batch - which might be undesirable in some use cases.

To quantify the rate of token mixing we investigated the evolution of the mean entropy of weights produced by Mixture of Tokens layers. In other words: in each layer, we average the entropy of weights over all token groups and all experts. 

For the base version of MoT, the results are as follows:

{{< include diagrams/_temp_baseline.qmd >}}

::: {.callout-tip}
## Backward Compatibility

The controller produces merging weights by taking a softmax over scores calculated independently for each token. If it should happen that one score would dominate, for example by having very low temperature, the mixing operation would focus on exclusively one token. This, in effect, would make Mixture of Tokens equivalent to Token-Choice architecture.

:::

As it turns out, allowing the temperature to be learnable, we effectively allow the model to lower rate of token mixing, especially in later layers:

{{< include diagrams/_temp_learnable.qmd >}}

Interestingly, this comes at a cost of model performance.

{{< include diagrams/_step_temp_loss.qmd >}}

Thus, allowing the temperature to be learned near the end of training seems to be a very promising direction for "private" autoregressive decoding: this way, we retain all the benefits of training with a high rate of token mixing, and have very low rates during inference.

[# TODO: dodać uczalną temperaturę na koniec treningu (te eksperymenty już mamy)]{.SA}



[obrazki z różnymi temperaturami softmax ]{.MK}


# Conclusions SJ
We have shown the preliminary results showing the promise of Mixture of Tokens improving both the stability of training and final performance of the model. More thorough experiments are underway at the moment and we plan to release the paper with more results in the coming weeks. In the meantime, you can contact us with any feedback you have at llm.random.team@gmail.com .[add twitter begging rwetweet us.]{.SJ}