---
title: "Continuous Mixture-of-Experts"
author:
    -   name: Szymon Antoniak \*
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Michał Krutul
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Maciej Pióro
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Jakub Krajewski
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Jan Ludziejewski
        equal-contributor: true
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Tomasz Odrzygóźdź
        affiliations:
        -   ref: ideas
        -   ref: uow
    -   name: Marek Cygan ‡
        affiliations:
        -   ref: uow
        -   ref: nomagic
    -   name: Sebastian Jaszczur †
        affiliations:
        -   ref: ideas
        -   ref: uow
affiliations:
    -   id: ideas
        name: IDEAS NCBR
    -   id: uow
        name: University of Warsaw
    -   id: nomagic
        name: Nomagic
date: "2023-09-29"
bibliography: bibliography.bib
# csl: apa.csl
# date-modified: "2023-09-29"
categories: []
image: "logo.png"
format:
  html:
    template-partials:
    - title-block.html
    include-in-header:
      - text: |
          <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.20.0.min.js"/>
---

###################################################################### POMYSŁ ###############################################################################

nazwa papera i techniki - Mixture of Tokens
!!!!!!!!!!!!!!!!

# Introduction (Abstract) SA
Mixture of Experts architectures have recently garnerned considerable attention for their ability to increase the size of Large Language Models while keeping the computational cost of training and inference constant. The most successful MoE approaches achieve this by activating only subsets of a very large feed forward layer for each processed token. This technique comes at a cost: the expert assignment operation must be discrete, which forces the use of gradient estimators for traditional gradient-based optimisation; the models are known to suffer from training issues such as training instability, under- and overload of _experts_ (subsets of the FeedForward layer) that lead to time overheads, leaving some tokens unprocessed, problems with expert exploration and specialization/redundancy, and finally communication costs in parallel training. While some of these problems can be alleviated with e.g. the use of various auxiliary losses or reduced initialisation scale, it is certain that existing MoE techniques are more difficult and less intuitive to train. With this motivation, we propose Mixture of Tokens: a new, fully-differentiable type of architecture that retains all efficiency benefits of MoE and alleviates the aformentioned problem by mixing _tokens_ before feeding them into the FeedForward layer. We would like to point that this architecture is also fully compatible with both masked and causual LLM training.

# Motivation SA
## MoE SA
Why MoE is cool. Why MoE is hard.


# Related Work MP
<!-- Co tu? Softmoe, Mixowanie ekspertów, Expert choice? I co jeszcze tutaj -->
In recent years, there has been a growing interest in methods of increasing the number of parameters in neural networks while maintaining their computational demands. A major advance in eliminating some problems with traditional MoE approaches is Expert Choice (here biblio). Our work has been inspired by their approach - Continuous MoE can be seen as an Expert Choice (as opposed to *Token Choice*) kind of method.

There is also concurrent work (Softmoe) investigating ideas similar to ours in the vision domain. It is, however, limited to encoder-only models. It is worth noting that earlier, other fully-differentiable MoE approaches had been proposed (biblio copied from Softmoe). /* jakoś inaczej sformułowac: However, expert-mixing strategies presented in those works incur significant computational overhead. Also, they are token choice instead of expert choice, so different than us. */

## Limitations of Current Approaches (sekcja 8.2 w paperze) SA 
<!-- JK - byłoby bardziej logicze dać tę sekcję pod related work -->
Without mentioning papers 
- obecne podejścia są niestabilne
- mamy idealny balancing token -> expert

# Method 
- Diagram SJ

## Implementation
<!-- MK -->
In the first phase of our Continuous-MoE Layer, we partition batch sequences into groups of tokens, ensuring each group is of equal size. 

For each group, we acquire a set of weights, named merge and emit. The sum of weights in both merge and emit equals 1 for every group, signifying the percentage of each token to utilize.

In every group, using merge weights, we consolidate each token group into a single token. Following this, we process all merged tokens with linear experts' weights and a relu activation function. Employing emit weights, we redistribute resultant tokens from the experts' layer back to a group of tokens. 

Lastly, we disassemble the groups to align with the Continuous-MoE input dimensions.
- encoder vs decoder MP

## Encoder vs Decoder
In encoder-only models, each token may attend to the whole input sequence, whereas decoder-only models only allow attending to the tokens that came before a particular token to simulate the autoregressive decoding used during model inference. This is crucial for the training of decoder-only models, as allowing a token to attend to future tokens would constitute a data leak, preventing successful training.
A major problem potentially facing any token-mixing strategy is the possibility of a data leak, making the strategy inappropriate for use with decoder-only models. In Continuous-MoE, the tokens are mixed across batch, i.e. with other tokens at the same position in other samples in the same training batch. We don't consider other strategies which would be more suitable in the context of encoders.


# Experiments MP / SA
## Experimental setup
In our experiments we train the models for $250{,}000$ steps using a cosine LR scheduler with $2500$ steps of warmup, and final LR equal to $0.1$ of the peak LR. The context length is $256$ tokens and the batch size is $256$.
We tune the max LR separately for each model.

## Co tam dalej
ContMoE vs dense vs expert choice (GPT-BERT mini)
Each run was optimized for that many batches of given size
Contmoe is better than dense


# On the Horizon
<!-- MK Kto wie czy to dobra nazwa, może Future Work lepsza -->
Coming up in the following weeks, we're ready to take our work up a notch with a few advancements that we're covering in this segment.

## Temperature annealing:
In base Continuous-MoE setup tokens from various samples in the batch are mingled. This theoretically opens doors for being able to tease out the output of one batch sample by another. However, this may pave the way for potential issues.

To tackle this problem, we plan to conduct an experiment where in the final phase of this experiment will involve a strategic, gradual reduction of temperature, hile simultaneously maintaining a low loss - a process referred to as 'temerature annealing'. The anticipated outcome is a discrete-routing model which promotes inference parallelization - a move expected to significantly boost the overall efficiency of our processes

## Scaling Up Models
In a previous section <TODO check> we showcased results for a BERT-mini FLOP matched Continues-MoE. Our preliminary experiments are also suggesting promising outcomes for larger models. In upcoming weeks we aim to prepare comprehensive comparison of all associated models.


## Do not forget about a BERT
As our results in this post covers only GPT model in future work we plan to compare resuts of Continues-MoE with other models on BERT. Our preliminary experiments are also suggesting promising outcomes for larger models. hus, we're gearing up to provide a thorough comparison of all associated models in upcoming weeks.


# Conclusions SJ
We have shown the preliminary results showing the promise of continuous Mixture of Experts improving both the stability of training and final performance of the model. More thorough experiments are underway at the moment and we will release the paper with more results in the coming weeks. In the meantime, you can contact us with any feedback you have at llm.random.team@gmail.com .